---
sidebar: sidebar 
permalink: express/express-direct-attach-aff220-deploy_deployment_procedures.html 
keywords: deployment, procedures, configure, flexpod, express, ip, based, storage, vmware, vsphere, setup, cisco, ucs, vcenter 
summary: このドキュメントでは、完全な冗長性と高可用性を備えた FlexPod Express システムの構成について詳しく説明します。 
---
= 導入手順
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


このドキュメントでは、完全な冗長性と高可用性を備えた FlexPod Express システムの構成について詳しく説明します。この冗長性を反映するために、各手順で設定するコンポーネントをコンポーネント A またはコンポーネント B と呼びますたとえば、このドキュメントでプロビジョニングされている 2 台のネットアップストレージコントローラは、コントローラ A とコントローラ B で識別されます。スイッチ A とスイッチ B は Cisco Nexus スイッチのペアを表します。ファブリックインターコネクト A とファブリックインターコネクト B は、 2 つの統合 Nexus ファブリックインターコネクトです。

また、このドキュメントでは、複数の Cisco UCS ホストをプロビジョニングする手順についても説明します。これらのホストは、サーバ A 、サーバ B などとして順次識別されます。

環境に関連する情報をステップに含める必要があることを示すために、コマンド構造の一部として「 \ <text>> 」が表示されます。「 vlan create 」コマンドについては、次の例を参照してください。

....
Controller01>vlan create vif0 <<mgmt_vlan_id>>
....
本ドキュメントでは、 FlexPod Express 環境を完全に構成する方法について説明します。このプロセスでは、さまざまな手順で、お客様固有の命名規則、 IP アドレス、および VLAN （仮想 LAN ）スキームを入力する必要があります。次の表に、このガイドで説明する導入に必要な VLAN を示します。このテーブルは、特定のサイト変数に基づいて作成し、ドキュメントの設定手順を実装するために使用できます。


NOTE: 別々のインバンド管理 VLAN とアウトオブバンド管理 VLAN を使用する場合は、それらの間にレイヤ 3 ルートを作成する必要があります。この検証では、共通の管理 VLAN を使用しました。

|===
| VLAN 名 | VLAN の目的 | このドキュメントの検証に使用する ID 


| 管理 VLAN | 管理インターフェイス用の VLAN | 18 


| ネイティブ VLAN | タグなしフレームが割り当てられている VLAN | 2. 


| NFS VLAN | NFS トラフィック用の VLAN | 104 


| VMware vMotion VLAN | ある物理ホストから別の物理ホストへの仮想マシン（ VM ）の移動用に指定された VLAN | 103 


| VM トラフィック VLAN | VM アプリケーショントラフィック用の VLAN | 102 


| iSCSI-A VLAN | ファブリック A の iSCSI トラフィック用 VLAN | 124 


| iSCSI-B VLAN | ファブリック B の iSCSI トラフィック用 VLAN | 125 
|===
VLAN 番号は、 FlexPod Express の設定全体で必要になります。VLAN は「 \<<var_xxxx_vlan>> 」と呼ばれます。「 xxxx 」は VLAN の目的（ iSCSI-A など）です。

次の表は、作成された VMware VM を示しています。

|===
| VM 概要の略 | ホスト名 


| VMware vCenter Server の各機能を使用し | Seahawks-vcsa.cie.netapp.com 
|===


== Cisco Nexus 31108PCV 導入手順

このセクションでは、 FlexPod Express 環境で使用される Cisco Nexus 31308PCV スイッチ構成について詳しく説明します。



=== Cisco Nexus 31108PCV スイッチの初期設定

ここでは、 FlexPod Express の基本環境で使用する Cisco Nexus スイッチの設定方法について説明します。


NOTE: この手順は、 NX-OS ソフトウェアリリース 7.0(3) I6(1) を実行する Cisco Nexus 31108PCV を使用していることを前提としています。

. スイッチのコンソールポートを最初にブートして接続すると、 Cisco NX-OS セットアップが自動的に開始されます。この初期構成では、スイッチ名、 mgmt0 インターフェイス構成、および Secure Shell （ SSH ）セットアップなどの基本的な設定を行います。
. FlexPod Express 管理ネットワークは、さまざまな方法で構成できます。31108PCV スイッチの mgmt0 インターフェイスは、既存の管理ネットワークに接続することも、 31108PCV スイッチの mgmt0 インターフェイスをバックツーバック構成で接続することもできる。ただし、このリンクは、 SSH トラフィックなどの外部管理アクセスには使用できません。
+
この導入ガイドでは、 FlexPod Express Cisco Nexus 31108PCV スイッチが既存の管理ネットワークに接続されています。

. Cisco Nexus 31108PCV スイッチを設定するには、スイッチの電源をオンにし、画面に表示される指示に従って両方のスイッチの初期セットアップを行い、スイッチ固有の情報に適切な値を置き換えます。
+
....
This setup utility will guide you through the basic configuration of the system. Setup configures only enough connectivity for management of the system.
....
+
....
*Note: setup is mainly used for configuring the system initially, when no configuration is present. So setup always assumes system defaults and not the current system configuration values.
Press Enter at anytime to skip a dialog. Use ctrl-c at anytime to skip the remaining dialogs.
Would you like to enter the basic configuration dialog (yes/no): y
Do you want to enforce secure password standard (yes/no) [y]: y
Create another login account (yes/no) [n]: n
Configure read-only SNMP community string (yes/no) [n]: n
Configure read-write SNMP community string (yes/no) [n]: n
Enter the switch name : 31108PCV-A
Continue with Out-of-band (mgmt0) management configuration? (yes/no) [y]: y
Mgmt0 IPv4 address : <<var_switch_mgmt_ip>>
Mgmt0 IPv4 netmask : <<var_switch_mgmt_netmask>>
Configure the default gateway? (yes/no) [y]: y
IPv4 address of the default gateway : <<var_switch_mgmt_gateway>>
Configure advanced IP options? (yes/no) [n]: n
Enable the telnet service? (yes/no) [n]: n
Enable the ssh service? (yes/no) [y]: y
Type of ssh key you would like to generate (dsa/rsa) [rsa]: rsa
Number of rsa key bits <1024-2048> [1024]: <enter>
Configure the ntp server? (yes/no) [n]: y
NTP server IPv4 address : <<var_ntp_ip>>
Configure default interface layer (L3/L2) [L2]: <enter>
Configure default switchport interface state (shut/noshut) [noshut]: <enter>
Configure CoPP system profile (strict/moderate/lenient/dense) [strict]: <enter>
....
. 設定の概要が表示され、設定を編集するかどうかを確認するメッセージが表示されます。設定が正しい場合は、「 n 」と入力します。
+
....
Would you like to edit the configuration? (yes/no) [n]: no
....
. その後、この設定を使用するかどうかを確認するメッセージが表示され、保存します。その場合は、「 y 」と入力します。
+
....
Use this configuration and save it? (yes/no) [y]: Enter
....
. Cisco Nexus スイッチ B について、手順 1~5 を繰り返します




=== 高度な機能を有効にします

追加の設定オプションを提供するには、 Cisco NX-OS で特定の高度な機能をイネーブルにする必要があります。

. Cisco Nexus スイッチ A およびスイッチ B で適切な機能をイネーブルにするには、コンフィギュレーションモードを開始するには、コマンド「（ config t ）」を使用し、次のコマンドを実行します。
+
....
feature interface-vlan
feature lacp
feature vpc
....
+

NOTE: ポートチャネルのデフォルトのロードバランシングハッシュでは、ソースおよびデスティネーションの IP アドレスを使用して、ポートチャネルのインターフェイス全体のロードバランシングアルゴリズムを決定します。ハッシュアルゴリズムにソースおよびデスティネーションの IP アドレス以外にもデータを提供することで、ポートチャネルのメンバー全体へのより均等なロードバランシングを実現できます。同じ理由から、ソースおよびデスティネーションの TCP ポートをハッシュアルゴリズムに追加することを推奨します。

. 構成モード（ config t ）から次のコマンドを実行し、 Cisco Nexus スイッチ A およびスイッチ B のグローバルポートチャネルロードバランシング構成を設定します。
+
....
port-channel load-balance src-dst ip-l4port
....




=== グローバルスパニングツリーコンフィギュレーションを実行します。

Cisco Nexus プラットフォームでは、ブリッジアシュアランスと呼ばれる新しい保護機能を使用します。ブリッジアシュアランスは、スパニングツリーアルゴリズムを実行していないデバイスでデータトラフィックの転送を継続する単方向リンクやその他のソフトウェア障害から保護するのに役立ちます。ポートは、プラットフォームに応じて、ネットワークやエッジなどのいくつかの状態のいずれかに配置できます。

すべてのポートがデフォルトでネットワークポートとみなされるように、ブリッジアシュアランスを設定することを推奨します。この設定により、ネットワーク管理者は各ポートの設定を確認することになります。また、未識別のエッジポートや、ブリッジアシュアランス機能が有効になっていないネイバーなど、最も一般的な構成エラーも表示されます。また、スパニングツリーでブロックするポートの数が少なすぎない方が、多くのポートをブロックする方が安全で、デフォルトのポートの状態でネットワーク全体の安定性を高めることができます。

サーバ、ストレージ、アップリンクスイッチを追加するときは、スパニングツリーの状態に細心の注意を払ってください。追加する構成がブリッジアシュアランスをサポートしていない場合は特に注意が必要です。このような場合は、ポートをアクティブにするためにポートタイプの変更が必要になることがあります。

Bridge Protocol Data Unit （ BPDU; ブリッジプロトコルデータユニット）ガードは、別の保護レイヤとしてデフォルトでエッジポートでイネーブルになっています。ネットワーク内のループを防止するために、このインターフェイス上で BPDU が別のスイッチから受信された場合、この機能はポートをシャットダウンします。

Cisco Nexus スイッチ A およびスイッチ B で、構成モード（「 config t 」）から次のコマンドを実行し、デフォルトのポートタイプや BPDU ガードなどのデフォルトのスパニングツリーオプションを設定します。

....
spanning-tree port type network default
spanning-tree port type edge bpduguard default
....


=== VLAN を定義します

VLAN の異なるポートを個別に設定する前に、レイヤ 2 VLAN をスイッチ上に定義する必要があります。また、 VLAN に名前を付けておくと、今後のトラブルシューティングを簡単に行うことができます。

コンフィギュレーションモード（ config t` ）から次のコマンドを実行して、 Cisco Nexus スイッチ A およびスイッチ B 上のレイヤ 2 VLAN を定義し、説明します。

....
vlan <<nfs_vlan_id>>
  name NFS-VLAN
vlan <<iSCSI_A_vlan_id>>
  name iSCSI-A-VLAN
vlan <<iSCSI_B_vlan_id>>
  name iSCSI-B-VLAN
vlan <<vmotion_vlan_id>>
  name vMotion-VLAN
vlan <<vmtraffic_vlan_id>>
  name VM-Traffic-VLAN
vlan <<mgmt_vlan_id>>
  name MGMT-VLAN
vlan <<native_vlan_id>>
  name NATIVE-VLAN
exit
....


=== アクセスポートと管理ポートの説明を設定します

レイヤ 2 VLAN に名前を割り当てる場合と同様に、すべてのインターフェイスに説明を設定すると、プロビジョニングとトラブルシューティングの両方に役立ちます。

各スイッチの構成モード（ config t ）から、 FlexPod Express の大規模構成の次のポート説明を入力します。



==== Cisco Nexus スイッチ A

....
int eth1/1
  description AFF A220-A e0M
int eth1/2
  description Cisco UCS FI-A mgmt0
int eth1/3
  description Cisco UCS FI-A eth1/1
int eth1/4
  description Cisco UCS FI-B eth1/1
int eth1/13
  description vPC peer-link 31108PVC-B 1/13
int eth1/14
  description vPC peer-link 31108PVC-B 1/14
....


==== Cisco Nexus スイッチ B

....
int eth1/1
  description AFF A220-B e0M
int eth1/2
  description Cisco UCS FI-B mgmt0
int eth1/3
  description Cisco UCS FI-A eth1/2
int eth1/4
  description Cisco UCS FI-B eth1/2
int eth1/13
  description vPC peer-link 31108PVC-B 1/13
int eth1/14
  description vPC peer-link 31108PVC-B 1/14
....


=== サーバおよびストレージの管理インターフェイスを設定します

サーバとストレージの管理インターフェイスで使用する VLAN は、通常、どちらも 1 つだけです。そのため、管理インターフェイスポートをアクセスポートとして設定します。各スイッチの管理 VLAN を定義し、スパニングツリーポートタイプをエッジに変更します。

構成モード (`config t`) から次のコマンドを実行して ' サーバとストレージの両方の管理インタフェースのポート設定を構成します



==== Cisco Nexus スイッチ A

....
int eth1/1-2
  switchport mode access
  switchport access vlan <<mgmt_vlan>>
  spanning-tree port type edge
  speed 1000
exit
....


==== Cisco Nexus スイッチ B

....
int eth1/1-2
  switchport mode access
  switchport access vlan <<mgmt_vlan>>
  spanning-tree port type edge
  speed 1000
exit
....


=== NTP 配信インターフェイスを追加します



==== Cisco Nexus スイッチ A

グローバルコンフィギュレーションモードから、次のコマンドを実行します。

....
interface Vlan<ib-mgmt-vlan-id>
ip address <switch-a-ntp-ip>/<ib-mgmt-vlan-netmask-length>
no shutdown
exitntp peer <switch-b-ntp-ip> use-vrf default
....


==== Cisco Nexus スイッチ B

グローバルコンフィギュレーションモードから、次のコマンドを実行します。

....
interface Vlan<ib-mgmt-vlan-id>
ip address <switch- b-ntp-ip>/<ib-mgmt-vlan-netmask-length>
no shutdown
exitntp peer <switch-a-ntp-ip> use-vrf default
....


=== 仮想ポートチャネルのグローバル設定を実行します

仮想ポートチャネル（ vPC ）を使用すると、 2 つの異なる Cisco Nexus スイッチに物理的に接続されたリンクを、 3 番目のデバイスに対する単一のポートチャネルとして認識できます。3 番目のデバイスには、スイッチ、サーバ、またはその他のネットワークデバイスを使用できます。vPC はレイヤ 2 マルチパスを提供します。これにより、帯域幅を増やし、ノード間で複数のパラレルパスを有効にし、代替パスが存在する場合はトラフィックをロードバランシングすることで、冗長性を確保できます。

vPC には次の利点があります。

* 1 つのデバイスが 2 つのアップストリームデバイス間でポートチャネルを使用できるようにする
* スパニングツリープロトコルのブロックポートの排除
* ループフリートポロジを提供する
* 使用可能なすべてのアップリンク帯域幅を使用する
* リンクまたはデバイスのいずれかに障害が発生した場合に、高速コンバージェンスを提供します
* リンクレベルの耐障害性を提供します
* 高可用性の実現を支援します


vPC 機能を正しく機能させるには、 2 つの Cisco Nexus スイッチ間でいくつかの初期セットアップを行う必要があります。バックツーバックの mgmt0 構成を使用する場合は、インターフェイスに定義されたアドレスを使用し、 ping `\<switch_a/B_mgmt0_ip_addr>> vrf ’ management コマンドを使用してそれらのアドレスで通信が可能であることを確認します。

構成モード（ config t ）から次のコマンドを実行し、両方のスイッチの vPC グローバル構成を設定します。



==== Cisco Nexus スイッチ A

....
vpc domain 1
 role priority 10
peer-keepalive destination <<switch_B_mgmt0_ip_addr>> source <<switch_A_mgmt0_ip_addr>> vrf management
  peer-gateway
  auto-recovery
  ip arp synchronize
  int eth1/13-14
  channel-group 10 mode active
int Po10description vPC peer-link
switchport
switchport mode trunkswitchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<nfs_vlan_id>>,<<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>, <<iSCSI_A_vlan_id>>, <<iSCSI_B_vlan_id>> spanning-tree port type network
vpc peer-link
no shut
exit
int Po13
description vPC ucs-FI-A
switchport mode trunk
switchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>> spanning-tree port type network
mtu 9216
vpc 13
no shut
exit
int eth1/3
  channel-group 13 mode active
int Po14
description vPC ucs-FI-B
switchport mode trunk
switchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>> spanning-tree port type network
mtu 9216
vpc 14
no shut
exit
int eth1/4
  channel-group 14 mode active
copy run start
....


==== Cisco Nexus スイッチ B

....
vpc domain 1
peer-switch
role priority 20
peer-keepalive destination <<switch_A_mgmt0_ip_addr>> source <<switch_B_mgmt0_ip_addr>> vrf management
  peer-gateway
  auto-recovery
  ip arp synchronize
  int eth1/13-14
  channel-group 10 mode active
int Po10
description vPC peer-link
switchport
switchport mode trunk
switchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<nfs_vlan_id>>,<<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>>, <<iSCSI_A_vlan_id>>, <<iSCSI_B_vlan_id>> spanning-tree port type network
vpc peer-link
no shut
exit
int Po13
description vPC ucs-FI-A
switchport mode trunk
switchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>> spanning-tree port type network
mtu 9216
vpc 13
no shut
exit
int eth1/3
  channel-group 13 mode active
int Po14
description vPC ucs-FI-B
switchport mode trunk
switchport trunk native vlan <<native_vlan_id>>
switchport trunk allowed vlan <<vmotion_vlan_id>>, <<vmtraffic_vlan_id>>, <<mgmt_vlan>> spanning-tree port type network
mtu 9216
vpc 14
no shut
exit
int eth1/4
  channel-group 14 mode active
copy run start
....

NOTE: この解決策検証では、最大伝送ユニット（ MTU ） 9 、 000 が使用されました。ただし、アプリケーションの要件に基づいて、適切な MTU 値を設定できます。FlexPod 解決策全体で同じ MTU 値を設定することが重要です。コンポーネント間の MTU 設定が正しくないと、パケットが破棄されます。



=== 既存のネットワークインフラへのアップリンク

使用可能なネットワークインフラに応じて、 FlexPod 環境をアップリンクするためのいくつかの方法や機能があります。既存の Cisco Nexus 環境がある場合は、 vPC を使用して、 FlexPod 環境に含まれる Cisco Nexus 31108PVC スイッチをインフラにアップリンクすることを推奨します。必要に応じて、 10GbE インフラ解決策の場合は 10GbE アップリンク、 1GbE インフラ解決策の場合は 1GbE アップリンクがサポートされます。前述の手順を使用して、既存の環境へのアップリンク vPC を作成できます。設定が完了したら、必ず copy run start を実行して各スイッチに設定を保存してください。



== ネットアップストレージ導入手順（パート 1 ）

このセクションでは、 NetApp AFF ストレージ導入手順について説明します。



=== NetApp ストレージコントローラ AFF2xx シリーズインストールガイド



==== NetApp Hardware Universe の略

。 https://hwu.netapp.com/Home/Index["NetApp Hardware Universe の略"^] （ HWU ）アプリケーションは、特定の ONTAP バージョンでサポートされているハードウェアコンポーネントとソフトウェアコンポーネントを提供します。ONTAP ソフトウェアで現在サポートされているネットアップのすべてのストレージアプライアンスに関する構成情報を提供します。また、コンポーネントの互換性の表も示します。

使用するハードウェアコンポーネントとソフトウェアコンポーネントが、インストールする ONTAP のバージョンでサポートされていることを確認します。

. にアクセスします http://hwu.netapp.com/Home/Index["HWU"^] システム設定ガイドを表示するアプリケーション。ストレージシステムの比較タブを選択して、 ONTAP ソフトウェアのバージョンとネットアップストレージアプライアンスの互換性を必要な仕様で確認します。
. または、ストレージアプライアンス別にコンポーネントを比較するには、ストレージシステムの比較をクリックします。


|===
| コントローラ AFF2XX シリーズの前提条件 


| ストレージシステムの物理的な場所を計画するには、次のセクションを参照してください。電力要件サポートされる電源コードオンボードポートとケーブル 
|===


==== ストレージコントローラ

のコントローラの物理的な設置手順に従います https://library-clnt.dmz.netapp.com/documentation/docweb/index.html?productID=62331&language=en-US["AFF A220 のドキュメント"^]。



=== NetApp ONTAP 9.5



==== 設定ワークシート

セットアップスクリプトを実行する前に、製品マニュアルから構成ワークシートに情報を記入してください。設定ワークシートは、で使用できます http://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-ssg/home.html["ONTAP 9.5 ソフトウェアセットアップガイド"^] （で使用できます http://docs.netapp.com/ontap-9/index.jsp["ONTAP 9 ドキュメンテーション・センター"^]）。次の表は、 ONTAP 9.5 のインストールと設定の情報を示しています。


NOTE: このシステムは、 2 ノードスイッチレスクラスタ構成でセットアップされます。

|===
| クラスタの詳細 | クラスタの値 


| クラスタノード A の IP アドレス | \<<var_nodeA_mgmt_ip>> 


| クラスタノード A のネットマスク | \<<var_nodeA_mgmt_mask>> を使用します 


| クラスタノード A のゲートウェイ | \<<var_nodeA_mgmt_gateway>> を使用します 


| クラスタノードの名前 | \<<var_nodeA>> を使用します 


| クラスタノード B の IP アドレス | \<<var_nodeB_mgmt_ip>> 


| クラスタノード B のネットマスク | \<<var_nodeB_mgmt_mask>> を使用します 


| クラスタノード B のゲートウェイ | \<<var_nodeB_mgmt_gateway>> を使用します 


| クラスタノード B の名前 | \<<var_nodeB>> を使用します 


| ONTAP 9.5 の URL | \<<var_url_boot_software>> を参照してください 


| クラスタの名前 | \<<var_clustername> を使用します 


| クラスタ管理 IP アドレス | \<<var_clustermgmt_ip>> 


| クラスタ B ゲートウェイ | \<<var_clustermgmt_gateway>> を使用します 


| クラスタ B のネットマスク | \<<var_clustermgmt_mask>> を使用します 


| ドメイン名 | \<<var_domain_name>> を参照してください 


| DNS サーバ IP （複数入力できます） | \<<var_dns_server_ip>> 


| NTP サーバ A の IP | <switch-A-ntp-ip>> 


| NTP サーバ B の IP | <switch-b-ntp-ip>> 
|===


==== ノード A を設定

ノード A を設定するには、次の手順を実行します。

. ストレージ・システムのコンソール・ポートに接続します。ローダー A のプロンプトが表示されます。ただし、ストレージシステムがリブートループに入っている場合は、このメッセージが表示されたら Ctrl-C キーを押して自動ブートループを終了します。
+
....
Starting AUTOBOOT press Ctrl-C to abort...
....
. システムをブートできるようにします。
+
....
autoboot
....
. Ctrl+C キーを押してブートメニューを表示します。
+
ONTAP 9 の場合：5 は起動しているソフトウェアのバージョンではありません。次の手順に進んで新しいソフトウェアをインストールしてください。ONTAP 9 の場合：5 はブートしているバージョンです。オプション 8 と y を選択してノードをリブートします。その後、手順 14 に進みます。

. 新しいソフトウェアをインストールするには ' オプション 7 を選択します
. アップグレードを実行するには 'y' を入力します
. ダウンロードに使用するネットワーク・ポートに e0M を選択します
. 今すぐ再起動するには 'y' を入力します
. e0M の IP アドレス、ネットマスク、およびデフォルトゲートウェイをそれぞれの場所に入力します。
+
....
<<var_nodeA_mgmt_ip>> <<var_nodeA_mgmt_mask>> <<var_nodeA_mgmt_gateway>>
....
. ソフトウェアを検索できる URL を入力します。
+

NOTE: ping 可能な Web サーバを指定する必要があります。

. ユーザ名が入力されていない場合は、 Enter キーを押します。
. 新しくインストールしたソフトウェアを ' 次回の再起動に使用するデフォルトとして設定するには 'y' を入力します
. ノードを再起動するには 'y' を入力します
+
新しいソフトウェアをインストールするときに、 BIOS およびアダプタカードのファームウェアアップグレードが実行され、リブートが発生してローダー A プロンプトで停止する可能性があります。これらの操作が行われた場合、システムがこの手順と異なることがあります。

. Ctrl+C キーを押してブートメニューを表示します。
. [Clean Configuration] で [4] を選択し、 [Initialize All Disks] を選択します。
. ディスクをゼロにするには 'y' を入力し ' 構成をリセットして ' 新しいファイル・システムをインストールします
. ディスク上のすべてのデータを消去するには 'y' を入力します
+
ルートアグリゲートの初期化と作成には、接続されているディスクの数とタイプに応じて 90 分以上かかる場合があります。初期化が完了すると、ストレージシステムがリブートします。SSD の初期化にかかる時間は大幅に短縮されます。ノード A のディスクの初期化中も、ノード B の設定を続行できます。

. ノード A を初期化している間に、ノード B の設定を開始します




==== ノード B を設定

ノード B を設定するには、次の手順を実行します。

. ストレージ・システムのコンソール・ポートに接続します。ローダー A のプロンプトが表示されます。ただし、ストレージシステムがリブートループに入っている場合は、このメッセージが表示されたら Ctrl-C キーを押して自動ブートループを終了します。
+
....
Starting AUTOBOOT press Ctrl-C to abort...
....
. Ctrl+C キーを押してブートメニューを表示します。
+
....
autoboot
....
. プロンプトが表示されたら、 Ctrl-C キーを押します。
+
ONTAP 9 の場合：5 は起動しているソフトウェアのバージョンではありません。次の手順に進んで新しいソフトウェアをインストールしてください。ブートしているバージョンが ONTAP 9.4 の場合は、オプション 8 と y を選択してノードをリブートします。その後、手順 14 に進みます。

. 新しいソフトウェアをインストールするには、オプション 7 を選択します。
. アップグレードを実行するには 'y' を入力します
. ダウンロードに使用するネットワーク・ポートに e0M を選択します
. 今すぐ再起動するには 'y' を入力します
. e0M の IP アドレス、ネットマスク、およびデフォルトゲートウェイをそれぞれの場所に入力します。
+
....
<<var_nodeB_mgmt_ip>> <<var_nodeB_mgmt_ip>><<var_nodeB_mgmt_gateway>>
....
. ソフトウェアを検索できる URL を入力します。
+

NOTE: ping 可能な Web サーバを指定する必要があります。

+
....
<<var_url_boot_software>>
....
. ユーザ名が入力されていない場合は、 Enter キーを押します
. 新しくインストールしたソフトウェアを ' 次回の再起動に使用するデフォルトとして設定するには 'y' を入力します
. ノードを再起動するには 'y' を入力します
+
新しいソフトウェアをインストールするときに、 BIOS およびアダプタカードのファームウェアアップグレードが実行され、リブートが発生してローダー A プロンプトで停止する可能性があります。これらの操作が行われた場合、システムがこの手順と異なることがあります。

. Ctrl+C キーを押してブートメニューを表示します。
. Clean Configuration および Initialize All Disks のオプション 4 を選択します。
. ディスクをゼロにするには 'y' を入力し ' 構成をリセットして ' 新しいファイル・システムをインストールします
. ディスク上のすべてのデータを消去するには 'y' を入力します
+
ルートアグリゲートの初期化と作成には、接続されているディスクの数とタイプに応じて 90 分以上かかる場合があります。初期化が完了すると、ストレージシステムがリブートします。SSD の初期化にかかる時間は大幅に短縮されます。





=== ノード A の設定およびクラスタ構成を継続します

ストレージコントローラ A （ノード A ）のコンソールポートに接続されているコンソールポートプログラムから、ノードセットアップスクリプトを実行します。このスクリプトは、 ONTAP 9.5 をノードで初めてブートしたときに表示されます。

ONTAP 9.5 では、ノードとクラスタのセットアップ手順が少し変更されています。クラスタセットアップウィザードを使用してクラスタの最初のノードを設定できるようになりました。 System Manager を使用してクラスタを設定します。

. プロンプトに従ってノード A をセットアップします
+
....
Welcome to the cluster setup wizard.
You can enter the following commands at any time:
  "help" or "?" - if you want to have a question clarified,
  "back" - if you want to change previously answered questions, and
  "exit" or "quit" - if you want to quit the cluster setup wizard.
     Any changes you made before quitting will be saved.
You can return to cluster setup at any time by typing "cluster setup".
To accept a default or omit a question, do not enter a value.
This system will send event messages and periodic reports to NetApp Technical Support. To disable this feature, enter
autosupport modify -support disable
within 24 hours.
Enabling AutoSupport can significantly speed problem determination and resolution should a problem occur on your system.
For further information on AutoSupport, see: http://support.netapp.com/autosupport/
Type yes to confirm and continue {yes}: yes
Enter the node management interface port [e0M]:
Enter the node management interface IP address: <<var_nodeA_mgmt_ip>>
Enter the node management interface netmask: <<var_nodeA_mgmt_mask>>
Enter the node management interface default gateway: <<var_nodeA_mgmt_gateway>>
A node management interface on port e0M with IP address <<var_nodeA_mgmt_ip>> has been created.
Use your web browser to complete cluster setup by accessing
https://<<var_nodeA_mgmt_ip>>
Otherwise, press Enter to complete cluster setup using the command line interface:
....
. ノードの管理インターフェイスの IP アドレスに移動します。
+

NOTE: クラスタのセットアップは、 CLI を使用して実行することもできます。このドキュメントでは、 NetApp System Manager のセットアップガイドを使用したクラスタセットアップについて説明します。

. クラスタを設定するには、セットアップガイドをクリックします。
. クラスタ名には「 \\<<var_clustername>> 」を、設定する各ノードには「 \<<var_nodeA>` 」と「 \<<var_nodeB>> 」を入力します。ストレージシステムに使用するパスワードを入力します。クラスタタイプに「スイッチレスクラスタ」を選択します。クラスタベースライセンスを入力します。
. クラスタ、 NFS 、および iSCSI の機能ライセンスを入力することもできます。
. クラスタの作成中を示すステータスメッセージが表示されます。このステータスメッセージは、複数のステータスを切り替えます。このプロセスには数分かかります。
. ネットワークを設定します
+
.. [IP Address Range] オプションを選択解除します。
.. Cluster Management IP Address フィールドに「 \<<var_clustermgmt_ip>> 」、 Netmask フィールドに「 \var_clustermgmt_mask>> 」と入力します。また、 Gateway フィールドに「 \<<var_clustermgmt_gateway>> 」と入力します。Port フィールドの ... セレクタを使用して、ノード A の e0M を選択します
.. ノード A のノード管理 IP がすでに入力されています。ノード B には '\\<<var_nodeA_mgmt_ip>> を入力します
.. [DNS Domain Name] フィールドに「 \<<var_domain_name>` 」と入力します。[DNS Server IP Address] フィールドに「 \<<var_dns_server_ip>> 」と入力します。
+
DNS サーバの IP アドレスは複数入力できます。

.. Primary NTP Server フィールドに「 \ <switch-a-ntp-ip>> 」と入力します。
+
代替 NTP サーバを「 \ <switch-b-ntp-ip>> 」として入力することもできます。



. サポート情報を設定します。
+
.. AutoSupport へのアクセスにプロキシが必要な環境の場合は、プロキシの URL をプロキシの URL に入力します。
.. イベント通知に使用する SMTP メールホストと E メールアドレスを入力します。
+
続行するには、少なくともイベント通知方式を設定する必要があります。いずれかの方法を選択できます。



. クラスタ構成が完了したことが示されたら、 Manage Your Cluster （クラスタの管理）をクリックしてストレージを構成します。




=== ストレージクラスタ構成を継続

ストレージノードとベースクラスタの設定が完了したら、ストレージクラスタの設定に進むことができます。



==== すべてのスペアディスクを初期化します

クラスタ内のすべてのスペアディスクを初期化するには、次のコマンドを実行します。

....
disk zerospares
....


==== オンボード UTA2 ポートパーソナリティを設定します

. ucadmin show コマンドを実行して、現在のモードとポートの現在のタイプを確認します。
+
....
AFFA220-Clus::> ucadmin show
                       Current  Current    Pending  Pending    Admin
Node          Adapter  Mode     Type       Mode     Type       Status
------------  -------  -------  ---------  -------  ---------  -----------
AFFA220-Clus-01
              0c       cna      target     -        -          offline
AFFA220-Clus-01
              0d       cna      target     -        -          offline
AFFA220-Clus-01
              0e       cna      target     -        -          offline
AFFA220-Clus-01
              0f       cna      target     -        -          offline
AFFA220-Clus-02
              0c       cna      target     -        -          offline
AFFA220-Clus-02
              0d       cna      target     -        -          offline
AFFA220-Clus-02
              0e       cna      target     -        -          offline
AFFA220-Clus-02
              0f       cna      target     -        -          offline
8 entries were displayed.
....
. 使用中のポートの現在のモードが「 cna 」であり、現在のタイプが「 target 」に設定されていることを確認します。設定されていない場合は、次のコマンドを実行してポートパーソナリティを変更します。
+
....
ucadmin modify -node <home node of the port> -adapter <port name> -mode cna -type target
....
+
前のコマンドを実行するには、ポートをオフラインにする必要があります。ポートをオフラインにするには、次のコマンドを実行します。

+
....
network fcp adapter modify -node <home node of the port> -adapter <port name> -state down
....
+

NOTE: ポートパーソナリティを変更した場合、変更を有効にするには、各ノードをリブートする必要があります。





==== Cisco Discovery Protocol を有効にします

ネットアップストレージコントローラで Cisco Discovery Protocol （ CDP ）を有効にするには、次のコマンドを実行します。

....
node run -node * options cdpd.enable on
....


==== すべてのイーサネットポートでリンクレイヤ検出プロトコルを有効にします

次のコマンドを実行して、ストレージスイッチとネットワークスイッチ間のリンクレイヤ検出プロトコル（ LLDP ）ネイバー情報の交換を有効にします。このコマンドは、クラスタ内のすべてのノードのすべてのポートで LLDP を有効にします。

....
node run * options lldp.enable on
....


==== 管理論理インターフェイスの名前を変更します

管理論理インターフェイス（ LIF ）の名前を変更するには、次の手順を実行します。

. 現在の管理 LIF の名前を表示します。
+
....
network interface show –vserver <<clustername>>
....
. クラスタ管理 LIF の名前を変更します。
+
....
network interface rename –vserver <<clustername>> –lif cluster_setup_cluster_mgmt_lif_1 –newname cluster_mgmt
....
. ノード B の管理 LIF の名前を変更します。
+
....
network interface rename -vserver <<clustername>> -lif cluster_setup_node_mgmt_lif_AFF A220_A_1 - newname AFF A220-01_mgmt1
....




==== クラスタ管理で自動リバートを設定する

クラスタ管理インターフェイスで 'auto-revert パラメータを設定します

....
network interface modify –vserver <<clustername>> -lif cluster_mgmt –auto-revert true
....


==== サービスプロセッサのネットワークインターフェイスをセットアップする

各ノードのサービスプロセッサに静的 IPv4 アドレスを割り当てるには、次のコマンドを実行します。

....
system service-processor network modify –node <<var_nodeA>> -address-family IPv4 –enable true – dhcp none –ip-address <<var_nodeA_sp_ip>> -netmask <<var_nodeA_sp_mask>> -gateway <<var_nodeA_sp_gateway>>
system service-processor network modify –node <<var_nodeB>> -address-family IPv4 –enable true – dhcp none –ip-address <<var_nodeB_sp_ip>> -netmask <<var_nodeB_sp_mask>> -gateway <<var_nodeB_sp_gateway>>
....

NOTE: サービスプロセッサの IP アドレスは、ノード管理 IP アドレスと同じサブネット内にある必要があります。



==== ONTAP でストレージフェイルオーバーを有効にします

ストレージフェイルオーバーが有効になっていることを確認するには、フェイルオーバーペアで次のコマンドを実行します。

. ストレージフェイルオーバーのステータスを確認
+
....
storage failover show
....
+
\\<<var_nodeA>>` と \\<<var_nodeB>> の両方がテイクオーバーを実行できる必要があります。ノードでテイクオーバーを実行できる場合は、ステップ 3 に進みます。

. 2 つのノードのどちらかでフェイルオーバーを有効にします。
+
....
storage failover modify -node <<var_nodeA>> -enabled true
....
. 2 ノードクラスタの HA ステータスを確認
+

NOTE: この手順は、ノードが 3 つ以上のクラスタには適用されません。

+
....
cluster ha show
....
. ハイアベイラビリティが構成されている場合は、ステップ 6 に進みます。ハイアベイラビリティが設定されている場合は、コマンドの実行時に次のメッセージが表示されます。
+
....
High Availability Configured: true
....
. HA モードは 2 ノードクラスタでのみ有効にします。
+
ノードが 3 つ以上のクラスタの場合は、このコマンドを実行しないでください。フェイルオーバーで問題が発生します。

+
....
cluster ha modify -configured true
Do you want to continue? {y|n}: y
....
. ハードウェアアシストが正しく設定されていることを確認し、必要に応じてパートナーの IP アドレスを変更
+
....
storage failover hwassist show
....
+
「 Keep Alive Status: Error: Did not receive hwassist keep alive alerts from partner 」というメッセージは、ハードウェアアシストが設定されていないことを示します。ハードウェアアシストを設定するには、次のコマンドを実行します。

+
....
storage failover modify –hwassist-partner-ip <<var_nodeB_mgmt_ip>> -node <<var_nodeA>>
storage failover modify –hwassist-partner-ip <<var_nodeA_mgmt_ip>> -node <<var_nodeB>>
....




==== ONTAP でジャンボフレーム MTU ブロードキャストドメインを作成します

MTU が 9000 のデータブロードキャストドメインを作成するには、次のコマンドを実行します。

....
broadcast-domain create -broadcast-domain Infra_NFS -mtu 9000
broadcast-domain create -broadcast-domain Infra_iSCSI-A -mtu 9000
broadcast-domain create -broadcast-domain Infra_iSCSI-B -mtu 9000
....


==== デフォルトのブロードキャストドメインからデータポートを削除します

10GbE のデータポートは iSCSI / NFS トラフィックに使用されます。これらのポートはデフォルトドメインから削除する必要があります。ポート e0e と e0f は使用されないため、デフォルトのドメインからも削除する必要があります。

ブロードキャストドメインからポートを削除するには、次のコマンドを実行します。

....
broadcast-domain remove-ports -broadcast-domain Default -ports <<var_nodeA>>:e0c, <<var_nodeA>>:e0d, <<var_nodeA>>:e0e, <<var_nodeA>>:e0f, <<var_nodeB>>:e0c, <<var_nodeB>>:e0d, <<var_nodeA>>:e0e, <<var_nodeA>>:e0f
....


==== UTA2 ポートではフロー制御を無効にします

ネットアップでは、外部デバイスに接続されているすべての UTA2 ポートでフロー制御を無効にすることをベストプラクティスとして推奨します。フロー制御を無効にするには、次のコマンドを実行します。

....
net port modify -node <<var_nodeA>> -port e0c -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeA>> -port e0d -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeA>> -port e0e -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeA>> -port e0f -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeB>> -port e0c -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeB>> -port e0d -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeB>> -port e0e -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
net port modify -node <<var_nodeB>> -port e0f -flowcontrol-admin none
Warning: Changing the network port settings will cause a several second interruption in carrier. Do you want to continue? {y|n}: y
....

NOTE: ONTAP への Cisco UCS Mini の直接接続は、 LACP をサポートしていません。



==== NetApp ONTAP でジャンボフレームを設定します

ジャンボフレーム（一般に MTU サイズが 9 、 000 バイトのフレーム）を使用するように ONTAP ネットワークポートを設定するには、クラスタシェルから次のコマンドを実行します。

....
AFF A220::> network port modify -node node_A -port e0e -mtu 9000
Warning: This command will cause a several second interruption of service on this network port.
Do you want to continue? {y|n}: y
AFF A220::> network port modify -node node_B -port e0e -mtu 9000
Warning: This command will cause a several second interruption of service on this network port.
Do you want to continue? {y|n}: y
AFF A220::> network port modify -node node_A -port e0f -mtu 9000
Warning: This command will cause a several second interruption of service on this network port.
Do you want to continue? {y|n}: y
AFF A220::> network port modify -node node_B -port e0f -mtu 9000
Warning: This command will cause a several second interruption of service on this network port.
Do you want to continue? {y|n}: y
....


==== ONTAP で VLAN を作成します

ONTAP で VLAN を作成するには、次の手順を実行します。

. NFS VLAN ポートを作成し、データブロードキャストドメインに追加します。
+
....
network port vlan create –node <<var_nodeA>> -vlan-name e0e-<<var_nfs_vlan_id>>
network port vlan create –node <<var_nodeA>> -vlan-name e0f-<<var_nfs_vlan_id>>
network port vlan create –node <<var_nodeB>> -vlan-name e0e-<<var_nfs_vlan_id>>
network port vlan create –node <<var_nodeB>> -vlan-name e0f-<<var_nfs_vlan_id>>
broadcast-domain add-ports -broadcast-domain Infra_NFS -ports <<var_nodeA>>: e0e- <<var_nfs_vlan_id>>, <<var_nodeB>>: e0e-<<var_nfs_vlan_id>> , <<var_nodeA>>:e0f- <<var_nfs_vlan_id>>, <<var_nodeB>>:e0f-<<var_nfs_vlan_id>>
....
. iSCSI VLAN ポートを作成し、データブロードキャストドメインに追加します。
+
....
network port vlan create –node <<var_nodeA>> -vlan-name e0e-<<var_iscsi_vlan_A_id>>
network port vlan create –node <<var_nodeA>> -vlan-name e0f-<<var_iscsi_vlan_B_id>>
network port vlan create –node <<var_nodeB>> -vlan-name e0e-<<var_iscsi_vlan_A_id>>
network port vlan create –node <<var_nodeB>> -vlan-name e0f-<<var_iscsi_vlan_B_id>>
broadcast-domain add-ports -broadcast-domain Infra_iSCSI-A -ports <<var_nodeA>>: e0e- <<var_iscsi_vlan_A_id>>,<<var_nodeB>>: e0e-<<var_iscsi_vlan_A_id>>
broadcast-domain add-ports -broadcast-domain Infra_iSCSI-B -ports <<var_nodeA>>: e0f- <<var_iscsi_vlan_B_id>>,<<var_nodeB>>: e0f-<<var_iscsi_vlan_B_id>>
....
. MGMT-VLAN ポートを作成します。
+
....
network port vlan create –node <<var_nodeA>> -vlan-name e0m-<<mgmt_vlan_id>>
network port vlan create –node <<var_nodeB>> -vlan-name e0m-<<mgmt_vlan_id>>
....




==== ONTAP でアグリゲートを作成する

ONTAP のセットアッププロセスで、ルートボリュームを含むアグリゲートが作成されます。追加のアグリゲートを作成するには、アグリゲート名、アグリゲートを作成するノード、アグリゲートに含まれるディスク数を確認します。

アグリゲートを作成するには、次のコマンドを実行します。

....
aggr create -aggregate aggr1_nodeA -node <<var_nodeA>> -diskcount <<var_num_disks>>
aggr create -aggregate aggr1_nodeB -node <<var_nodeB>> -diskcount <<var_num_disks>>
....
構成内で少なくとも 1 つのディスクをスペアとして保持します（最も大きいディスクを選択してください）。ディスクのタイプとサイズごとに少なくとも 1 つのスペアを用意しておくことを推奨します。

ディスクは 5 本から始めて、追加のストレージが必要になったときにアグリゲートにディスクを追加できます。

ディスクの初期化が完了するまで、アグリゲートを作成することはできません。aggr show コマンドを実行して、アグリゲートの作成ステータスを表示します。「 aggr1_nodeA 」がオンラインになるまで、次の手順に進まないでください。



==== ONTAP でタイムゾーンを設定します

時刻の同期を設定し、クラスタのタイムゾーンを設定するには、次のコマンドを実行します。

....
timezone <<var_timezone>>
....

NOTE: たとえば、米国東部では、タイムゾーンは「アメリカ / ニューヨーク」です。タイムゾーン名の入力を開始したら、 Tab キーを押して使用可能なオプションを表示します。



==== ONTAP で SNMP を設定します

SNMP を設定するには、次の手順を実行します。

. 場所や連絡先などの SNMP 基本情報を設定します。ポーリング時に ' この情報は 'sysLocation' 変数と SNMP の sysContact' 変数として表示されます
+
....
snmp contact <<var_snmp_contact>>
snmp location “<<var_snmp_location>>”
snmp init 1
options snmp.enable on
....
. リモートホストに送信する SNMP トラップを設定します。
+
....
snmp traphost add <<var_snmp_server_fqdn>>
....




==== ONTAP で SNMPv1 を設定します

SNMPv1 を設定するには、コミュニティと呼ばれる共有シークレットのプレーンテキストパスワードを設定します。

....
snmp community add ro <<var_snmp_community>>
....

NOTE: 「 snmp community delete all 」コマンドは慎重に使用してください。他の監視製品にコミュニティストリングが使用されている場合、このコマンドはそれらを削除します。



==== ONTAP で SNMPv3 を設定します

SNMPv3 では、認証用のユーザを定義および設定する必要があります。SNMPv3 を設定するには、次の手順を実行します。

. 「 securitysnmpusers 」コマンドを実行して、エンジン ID を表示します。
. 「 mpv3user 」という名前のユーザを作成します。
+
....
security login create -username snmpv3user -authmethod usm -application snmp
....
. 信頼できるエンティティのエンジン ID を入力し、認証プロトコルとして「 mD5 」を選択します。
. プロンプトが表示されたら、認証プロトコルのパスワードとして最低 8 文字のパスワードを入力します。
. プライバシープロトコルとして「 es 」を選択します。
. プロンプトが表示されたら、プライバシープロトコルのパスワードとして最低 8 文字のパスワードを入力します。




==== ONTAP で AutoSupport HTTPS を設定します

NetApp AutoSupport ツールは、サポート概要情報を HTTPS 経由でネットアップに送信します。AutoSupport を設定するには、次のコマンドを実行します。

....
system node autosupport modify -node * -state enable –mail-hosts <<var_mailhost>> -transport https -support enable -noteto <<var_storage_admin_email>>
....


==== Storage Virtual Machine を作成

インフラ Storage Virtual Machine （ SVM ）を作成するには、次の手順を実行します。

. vserver create コマンドを実行します
+
....
vserver create –vserver Infra-SVM –rootvolume rootvol –aggregate aggr1_nodeA –rootvolume- security-style unix
....
. NetApp VSC のインフラ SVM アグリゲートリストにデータアグリゲートを追加します。
+
....
vserver modify -vserver Infra-SVM -aggr-list aggr1_nodeA,aggr1_nodeB
....
. NFS と iSCSI を残して、未使用のストレージプロトコルを SVM から削除します。
+
....
vserver remove-protocols –vserver Infra-SVM -protocols cifs,ndmp,fcp
....
. インフラ SVM で NFS プロトコルを有効にして実行します。
+
....
nfs create -vserver Infra-SVM -udp disabled
....
. NetApp NFS VAAI プラグインの「 VM vStorage 」パラメータをオンにします。次に、 NFS が設定されていることを確認します。
+
....
vserver nfs modify –vserver Infra-SVM –vstorage enabled
vserver nfs show
....
+

NOTE: SVM は以前はサーバと呼ばれていたため、コマンドラインでは「 vserver 」の前にコマンドが配置されます





==== ONTAP で NFSv3 を設定します

次の表に、この設定を完了するために必要な情報を示します。

|===
| 詳細（ Detail ） | 詳細値 


| ESXi ホスト A の NFS IP アドレス | \<<var_esxi_hostA_nfs_ip>> 


| ESXi ホスト B の NFS IP アドレス | \<<var_esxi_hostB_nfs_ip>> を追加します 
|===
SVM に NFS を設定するには、次のコマンドを実行します。

. デフォルトのエクスポートポリシーに各 ESXi ホスト用のルールを作成します。
. 作成する各 ESXi ホストにルールを割り当てます。各ホストには独自のルールインデックスがあります。最初の ESXi ホストのルールインデックスは 1 、 2 番目の ESXi ホストのルールインデックスは 2 のようになります。
+
....
vserver export-policy rule create –vserver Infra-SVM -policyname default –ruleindex 1 –protocol nfs -clientmatch <<var_esxi_hostA_nfs_ip>> -rorule sys –rwrule sys -superuser sys –allow-suid falsevserver export-policy rule create –vserver Infra-SVM -policyname default –ruleindex 2 –protocol nfs -clientmatch <<var_esxi_hostB_nfs_ip>> -rorule sys –rwrule sys -superuser sys –allow-suid false
vserver export-policy rule show
....
. エクスポートポリシーをインフラ SVM ルートボリュームに割り当てます。
+
....
volume modify –vserver Infra-SVM –volume rootvol –policy default
....
+

NOTE: エクスポートポリシーは、 vSphere のセットアップ後にインストールするように選択した場合に自動的に処理されます。インストールしない場合は、 Cisco UCS B シリーズサーバを追加するときにエクスポートポリシールールを作成する必要があります。





==== ONTAP で iSCSI サービスを作成します

iSCSI サービスを作成するには、次の手順を実行します。

. SVM で iSCSI サービスを作成します。また、このコマンドでは iSCSI サービスが開始され、 SVM に iSCSI Qualified Name （ IQN ）が設定されます。iSCSI が設定されていることを確認します。
+
....
iscsi create -vserver Infra-SVM
iscsi show
....




==== ONTAP で SVM ルートボリュームの負荷共有ミラーを作成

ONTAP で SVM ルートボリュームの負荷共有ミラーを作成するには、次の手順を実行します。

. インフラ SVM ルートボリュームの負荷共有ミラーとなるボリュームを各ノードに作成します。
+
....
volume create –vserver Infra_Vserver –volume rootvol_m01 –aggregate aggr1_nodeA –size 1GB –type DPvolume create –vserver Infra_Vserver –volume rootvol_m02 –aggregate aggr1_nodeB –size 1GB –type DP
....
. ルートボリュームのミラー関係を 15 分ごとに更新するジョブスケジュールを作成します。
+
....
job schedule interval create -name 15min -minutes 15
....
. ミラーリング関係を作成
+
....
snapmirror create -source-path Infra-SVM:rootvol -destination-path Infra-SVM:rootvol_m01 -type LS -schedule 15min
snapmirror create -source-path Infra-SVM:rootvol -destination-path Infra-SVM:rootvol_m02 -type LS -schedule 15min
....
. ミラーリング関係を初期化し、作成されたことを確認します。
+
....
snapmirror initialize-ls-set -source-path Infra-SVM:rootvol snapmirror show
....




==== ONTAP で HTTPS アクセスを設定する

ストレージコントローラへのセキュアなアクセスを設定するには、次の手順を実行します。

. 証明書コマンドにアクセスするには、権限レベルを上げてください。
+
....
set -privilege diag
Do you want to continue? {y|n}: y
....
. 通常は、自己署名証明書がすでに存在します。次のコマンドを実行して証明書を確認します。
+
....
security certificate show
....
. 表示されている各 SVM の証明書の共通名は、 SVM の DNS 完全修飾ドメイン名（ FQDN ）と一致している必要があります。4 つのデフォルト証明書を削除して、認証局の自己署名証明書または証明書に置き換える必要があります。
+
証明書を作成する前に期限切れになった証明書を削除することを推奨します。「 securitycertificate delete 」コマンドを実行して、期限切れの証明書を削除します。次のコマンドでは、タブ補完を使用して、デフォルトの証明書を選択して削除します。

+
....
security certificate delete [TAB] ...
Example: security certificate delete -vserver Infra-SVM -common-name Infra-SVM -ca Infra-SVM - type server -serial 552429A6
....
. 自己署名証明書を生成してインストールするには、次のコマンドを 1 回限りのコマンドとして実行します。インフラ SVM とクラスタ SVM のサーバ証明書を生成します。これらのコマンドの実行に役立つように、タブ補完を使用してください。
+
....
security certificate create [TAB] ...
Example: security certificate create -common-name infra-svm.netapp.com -type server -size 2048 - country US -state "North Carolina" -locality "RTP" -organization "NetApp" -unit "FlexPod" -email- addr "abc@netapp.com" -expire-days 365 -protocol SSL -hash-function SHA256 -vserver Infra-SVM
....
. 次の手順で必要なパラメータの値を取得するには、「 securitycertificate show 」コマンドを実行します。
. 作成した各証明書を ' – server-enabled true' および– client-enabled false' パラメータを使用して有効にしますタブ補完を使用してください。
+
....
security ssl modify [TAB] ...
Example: security ssl modify -vserver Infra-SVM -server-enabled true -client-enabled false -ca infra-svm.netapp.com -serial 55243646 -common-name infra-svm.netapp.com
....
. SSL と HTTPS アクセスを設定して有効にし、 HTTP アクセスを無効にします。
+
....
system services web modify -external true -sslv3-enabled true
Warning: Modifying the cluster configuration will cause pending web service requests to be interrupted as the web servers are restarted.
Do you want to continue {y|n}: y
System services firewall policy delete -policy mgmt -service http -vserver <<var_clustername>>
....
+

NOTE: これらのコマンドの一部で、エントリが存在しないことを示すエラーメッセージが返されますが、これは通常の動作であり問題ありません。

. admin 権限レベルにリバートしてセットアップを作成し、 SVM を Web で使用できるようにします。
+
....
set –privilege admin
vserver services web modify –name spi|ontapi|compat –vserver * -enabled true
....




==== ONTAP で NetApp FlexVol ボリュームを作成します

NetApp FlexVol ® ボリュームを作成するには、ボリューム名、サイズ、およびボリュームが存在するアグリゲートを入力します。2 つの VMware データストアボリュームと 1 つのサーバブートボリュームを作成します。

....
volume create -vserver Infra-SVM -volume infra_datastore_1 -aggregate aggr1_nodeA -size 500GB - state online -policy default -junction-path /infra_datastore_1 -space-guarantee none -percent- snapshot-space 0
volume create -vserver Infra-SVM -volume infra_datastore_2 -aggregate aggr1_nodeB -size 500GB - state online -policy default -junction-path /infra_datastore_2 -space-guarantee none -percent- snapshot-space 0
....
....
volume create -vserver Infra-SVM -volume infra_swap -aggregate aggr1_nodeA -size 100GB -state online -policy default -juntion-path /infra_swap -space-guarantee none -percent-snapshot-space 0 -snapshot-policy none
volume create -vserver Infra-SVM -volume esxi_boot -aggregate aggr1_nodeA -size 100GB -state online -policy default -space-guarantee none -percent-snapshot-space 0
....


==== ONTAP で重複排除を有効にします

適切なボリュームで 1 日に 1 回重複排除を有効にするには、次のコマンドを実行します。

....
volume efficiency modify –vserver Infra-SVM –volume esxi_boot –schedule sun-sat@0
volume efficiency modify –vserver Infra-SVM –volume infra_datastore_1 –schedule sun-sat@0
volume efficiency modify –vserver Infra-SVM –volume infra_datastore_2 –schedule sun-sat@0
....


==== ONTAP で LUN を作成します

2 つのブート論理ユニット番号（ LUN ）を作成するには、次のコマンドを実行します。

....
lun create -vserver Infra-SVM -volume esxi_boot -lun VM-Host-Infra-A -size 15GB -ostype vmware - space-reserve disabled
lun create -vserver Infra-SVM -volume esxi_boot -lun VM-Host-Infra-B -size 15GB -ostype vmware - space-reserve disabled
....

NOTE: Cisco UCS C シリーズサーバを追加する場合は、追加のブート LUN を作成する必要があります。



==== ONTAP に iSCSI LIF を作成

次の表に、この設定を完了するために必要な情報を示します。

|===
| 詳細（ Detail ） | 詳細値 


| ストレージノード A iSCSI LIF01A | \<<var_nodeA_iscsi_lif01a_ip>> 


| ストレージノード A の iSCSI LIF01A ネットワークマスク | \<<var_nodeA_iscsi_lif01a _mask>> をクリックします 


| ストレージノード A iSCSI LIF01B | \<<var_nodeA_iscsi_lif01b_ip>> 


| ストレージノード A の iSCSI LIF01B ネットワークマスク | \<<var_nodeA_iscsi_lif01b_mask>> をクリックします 


| ストレージノード B iSCSI LIF01A | \<<var_nodeB_iscsi_lif01a_ip>> 


| ストレージノード B iSCSI LIF01A ネットワークマスク | \<<var_nodeB_iscsi_lif01a_mask>> を選択します 


| ストレージノード B iSCSI LIF01B | \<<var_nodeB_iscsi_lif01b_ip>> 


| ストレージノード B iSCSI LIF01B ネットワークマスク | \<<var_nodeB_iscsi_lif01b_mask>> をクリックします 
|===
. 各ノードに 2 つずつ、 4 つの iSCSI LIF を作成します。
+
....
network interface create -vserver Infra-SVM -lif iscsi_lif01a -role data -data-protocol iscsi - home-node <<var_nodeA>> -home-port e0e-<<var_iscsi_vlan_A_id>> -address <<var_nodeA_iscsi_lif01a_ip>> -netmask <<var_nodeA_iscsi_lif01a_mask>> –status-admin up – failover-policy disabled –firewall-policy data –auto-revert false
network interface create -vserver Infra-SVM -lif iscsi_lif01b -role data -data-protocol iscsi - home-node <<var_nodeA>> -home-port e0f-<<var_iscsi_vlan_B_id>> -address <<var_nodeA_iscsi_lif01b_ip>> -netmask <<var_nodeA_iscsi_lif01b_mask>> –status-admin up – failover-policy disabled –firewall-policy data –auto-revert false
network interface create -vserver Infra-SVM -lif iscsi_lif02a -role data -data-protocol iscsi - home-node <<var_nodeB>> -home-port e0e-<<var_iscsi_vlan_A_id>> -address <<var_nodeB_iscsi_lif01a_ip>> -netmask <<var_nodeB_iscsi_lif01a_mask>> –status-admin up – failover-policy disabled –firewall-policy data –auto-revert false
network interface create -vserver Infra-SVM -lif iscsi_lif02b -role data -data-protocol iscsi - home-node <<var_nodeB>> -home-port e0f-<<var_iscsi_vlan_B_id>> -address <<var_nodeB_iscsi_lif01b_ip>> -netmask <<var_nodeB_iscsi_lif01b_mask>> –status-admin up – failover-policy disabled –firewall-policy data –auto-revert false
network interface show
....




==== ONTAP に NFS LIF を作成します

次の表に、この設定を完了するために必要な情報を示します。

|===
| 詳細（ Detail ） | 詳細値 


| ストレージノード A NFS LIF 01 A IP | \<<var_nodeA_nfs_lif_01_a_ip>> 


| ストレージノード A NFS LIF 01 のネットワークマスク | \<<var_nodeA_nfs_lif_01_a_mask>> を参照してください 


| ストレージノード A NFS LIF 01 b IP | \<<var_nodeA_nfs_lif_01_b_ip>> 


| ストレージノード A NFS LIF 01 b ネットワークマスク | \<<var_nodeA_nfs_lif_01_b_mask>> を参照してください 


| ストレージノード B の NFS LIF 02 A IP | \<<var_nodeB_nfs_lif_02_a_ip>> 


| ストレージノード B の NFS LIF 02 A ネットワークマスク | \<<var_nodeB_nfs_lif_02.a_mask>> を参照してください 


| ストレージノード B の NFS LIF 02 b IP | \<<var_nodeB_nfs_lif_02_b_ip>> 


| ストレージノード B の NFS LIF 02 b ネットワークマスク | \<<var_nodeB_nfs_lif_02_b_mask>> を参照してください 
|===
. NFS LIF を作成します。
+
....
network interface create -vserver Infra-SVM -lif nfs_lif01_a -role data -data-protocol nfs -home- node <<var_nodeA>> -home-port e0e-<<var_nfs_vlan_id>> –address <<var_nodeA_nfs_lif_01_a_ip>> - netmask << var_nodeA_nfs_lif_01_a_mask>> -status-admin up –failover-policy broadcast-domain-wide – firewall-policy data –auto-revert true
network interface create -vserver Infra-SVM -lif nfs_lif01_b -role data -data-protocol nfs -home- node <<var_nodeA>> -home-port e0f-<<var_nfs_vlan_id>> –address <<var_nodeA_nfs_lif_01_b_ip>> - netmask << var_nodeA_nfs_lif_01_b_mask>> -status-admin up –failover-policy broadcast-domain-wide – firewall-policy data –auto-revert true
network interface create -vserver Infra-SVM -lif nfs_lif02_a -role data -data-protocol nfs -home- node <<var_nodeB>> -home-port e0e-<<var_nfs_vlan_id>> –address <<var_nodeB_nfs_lif_02_a_ip>> - netmask << var_nodeB_nfs_lif_02_a_mask>> -status-admin up –failover-policy broadcast-domain-wide – firewall-policy data –auto-revert true
network interface create -vserver Infra-SVM -lif nfs_lif02_b -role data -data-protocol nfs -home- node <<var_nodeB>> -home-port e0f-<<var_nfs_vlan_id>> –address <<var_nodeB_nfs_lif_02_b_ip>> - netmask << var_nodeB_nfs_lif_02_b_mask>> -status-admin up –failover-policy broadcast-domain-wide – firewall-policy data –auto-revert true
network interface show
....




==== インフラ SVM 管理者を追加

次の表に、この設定を完了するために必要な情報を示します。

|===
| 詳細（ Detail ） | 詳細値 


| vsmgmt IP | \<<var_svm_mgmt_ip>> を追加します 


| vsmgmt ネットワークマスク | \<<var_SVM_mgmt_mask>> を使用します 


| vsmgmt デフォルトゲートウェイ | \<<var_SVM_mgmt_gateway>> を使用します 
|===
インフラ SVM 管理者および SVM 管理 LIF を管理ネットワークに追加するには、次の手順を実行します。

. 次のコマンドを実行します。
+
....
network interface create –vserver Infra-SVM –lif vsmgmt –role data –data-protocol none –home-node <<var_nodeB>> -home-port e0M –address <<var_svm_mgmt_ip>> -netmask <<var_svm_mgmt_mask>> - status-admin up –failover-policy broadcast-domain-wide –firewall-policy mgmt –auto-revert true
....
+

NOTE: ここで指定する SVM 管理 IP は、ストレージクラスタ管理 IP と同じサブネット内にある必要があります。

. SVM 管理インターフェイスの外部へのアクセスを許可するデフォルトルートを作成します。
+
....
network route create –vserver Infra-SVM -destination 0.0.0.0/0 –gateway <<var_svm_mgmt_gateway>> network route show
....
. SVM 「 vsadmin 」ユーザのパスワードを設定し、ユーザのロックを解除します。
+
....
security login password –username vsadmin –vserver Infra-SVM
Enter a new password: <<var_password>>
Enter it again: <<var_password>>
security login unlock –username vsadmin –vserver
....




== Cisco UCS サーバの構成



=== FlexPod の Cisco UCS ベース

FlexPod 環境で Cisco UCS 6324 ファブリックインターコネクトの初期セットアップを実行します。

このセクションでは、 Cisco UCS Manger を使用して、 FlexPod ROBO 環境で使用する Cisco UCS を設定する手順について詳しく説明します。



=== Cisco UCS ファブリックインターコネクト 6324 A

Cisco UCS は、アクセスレイヤネットワークとサーバを使用します。この高性能な次世代サーバシステムは、データセンターにワークロードの即応性と拡張性をもたらします。

Cisco UCS Manager 4.0(1b) は、ファブリックインターコネクトを Cisco UCS シャーシに統合する 6324 ファブリックインターコネクトをサポートし、より小規模な導入環境に解決策を統合します。Cisco UCS Mini により、システム管理が簡素化され、低規模な導入のためのコストが削減されます。

ハードウェアコンポーネントとソフトウェアコンポーネントは、シスコのユニファイドファブリックをサポートしています。ユニファイドファブリックは、単一の統合ネットワークアダプタ上で複数のタイプのデータセンタートラフィックを処理します。



=== システムの初期セットアップ

Cisco UCS ドメイン内のファブリックインターコネクトに初めてアクセスすると、セットアップウィザードによって、システムの設定に必要な次の情報の入力が求められます。

* インストール方法（ GUI または CLI ）
* セットアップモード（フルシステムバックアップまたは初期セットアップからリストア）
* システム構成の種類（スタンドアロンまたはクラスタ構成）
* システム名
* 管理パスワード
* 管理ポートの IPv4 アドレスとサブネットマスク、または IPv6 アドレスとプレフィックス
* デフォルトゲートウェイの IPv4 アドレスまたは IPv6 アドレス
* DNS サーバの IPv4 アドレスまたは IPv6 アドレス
* デフォルトのドメイン名


次の表に、 Fabric Interconnect A で Cisco UCS の初期設定を完了するために必要な情報を示します

|===
| 詳細（ Detail ） | 詳細 / 値 


| システム名  | \<<var_UCS_clustername> を使用します 


| 管理パスワード | \<<var_password>> 


| 管理 IP アドレス：ファブリックインターコネクト A | \<<var_ucsa_mgmt_ip>> を追加します 


| 管理ネットマスク： Fabric Interconnect A | \<<var_ucsa_mgmt_mask>> を使用します 


| デフォルトゲートウェイ： Fabric Interconnect A | \<<var_ucsa_mgmt_gateway>> を使用します 


| クラスタの IP アドレス | \<<var_UCS_cluster_ip>> 


| DNS サーバの IP アドレス | \<<var_nameserver_ip>> 


| ドメイン名 | \<<var_domain_name>> を参照してください 
|===
FlexPod 環境で使用するように Cisco UCS を設定するには、次の手順を実行します。

. 最初の Cisco UCS 6324 ファブリックインターコネクト A のコンソールポートに接続します
+
....
Enter the configuration method. (console/gui) ? console

  Enter the setup mode; setup newly or restore from backup. (setup/restore) ? setup

  You have chosen to setup a new Fabric interconnect. Continue? (y/n): y

  Enforce strong password? (y/n) [y]: Enter

  Enter the password for "admin":<<var_password>>
  Confirm the password for "admin":<<var_password>>

  Is this Fabric interconnect part of a cluster(select 'no' for standalone)? (yes/no) [n]: yes

  Enter the switch fabric (A/B) []: A

  Enter the system name: <<var_ucs_clustername>>

  Physical Switch Mgmt0 IP address : <<var_ucsa_mgmt_ip>>

  Physical Switch Mgmt0 IPv4 netmask : <<var_ucsa_mgmt_mask>>

  IPv4 address of the default gateway : <<var_ucsa_mgmt_gateway>>

  Cluster IPv4 address : <<var_ucs_cluster_ip>>

  Configure the DNS Server IP address? (yes/no) [n]: y

       DNS IP address : <<var_nameserver_ip>>

  Configure the default domain name? (yes/no) [n]: y
Default domain name: <<var_domain_name>>

  Join centralized management environment (UCS Central)? (yes/no) [n]: no

 NOTE: Cluster IP will be configured only after both Fabric Interconnects are initialized. UCSM will be functional only after peer FI is configured in clustering mode.

  Apply and save the configuration (select 'no' if you want to re-enter)? (yes/no): yes
  Applying configuration. Please wait.

  Configuration file - Ok
....
. コンソールに表示される設定を確認します。正しい場合は、回答は設定を適用して保存します。
. ログインプロンプトで設定が保存されたことを確認します。


次の表に、ファブリックインターコネクト B で Cisco UCS の初期設定を完了するために必要な情報を示します

|===
| 詳細（ Detail ） | 詳細 / 値 


| システム名  | \<<var_UCS_clustername> を使用します 


| 管理パスワード | \<<var_password>> 


| 管理 IP アドレス - FI B | \<<var_UCSB_mgmt_ip>> を追加します 


| 管理ネットマスク - FI B | \<<var_UCSB_mgmt_mask>> を使用します 


| デフォルトゲートウェイ - FI B | \<<var_UCSB_mgmt_gateway>> を使用します 


| クラスタの IP アドレス | \<<var_UCS_cluster_ip>> 


| DNS サーバの IP アドレス | \<<var_nameserver_ip>> 


| ドメイン名（ Domain Name ） | \<<var_domain_name>> を参照してください 
|===
. 2 番目の Cisco UCS 6324 ファブリックインターコネクト B のコンソールポートに接続します
+
....
 Enter the configuration method. (console/gui) ? console

  Installer has detected the presence of a peer Fabric interconnect. This Fabric interconnect will be added to the cluster. Continue (y/n) ? y

  Enter the admin password of the peer Fabric interconnect:<<var_password>>
    Connecting to peer Fabric interconnect... done
    Retrieving config from peer Fabric interconnect... done
    Peer Fabric interconnect Mgmt0 IPv4 Address: <<var_ucsb_mgmt_ip>>
    Peer Fabric interconnect Mgmt0 IPv4 Netmask: <<var_ucsb_mgmt_mask>>
    Cluster IPv4 address: <<var_ucs_cluster_address>>

    Peer FI is IPv4 Cluster enabled. Please Provide Local Fabric Interconnect Mgmt0 IPv4 Address

  Physical Switch Mgmt0 IP address : <<var_ucsb_mgmt_ip>>


  Apply and save the configuration (select 'no' if you want to re-enter)? (yes/no): yes
  Applying configuration. Please wait.

  Configuration file - Ok
....
. ログインプロンプトで、設定が保存されたことを確認します。




=== Cisco UCS Manager にログインします。

Cisco Unified Computing System （ UCS ）環境にログインするには、次の手順を実行します。

. Web ブラウザを開き、 Cisco UCS ファブリックインターコネクトクラスタのアドレスに移動します。
+
Cisco UCS Manager が起動するように 2 つ目のファブリックインターコネクトを設定した後、 5 分以上待つ必要があります。

. Launch UCS Manager リンクをクリックして、 Cisco UCS Manager を起動します。
. 必要なセキュリティ証明書を受け入れます。
. プロンプトが表示されたら、ユーザ名に admin を入力し、管理者パスワードを入力します。
. Login をクリックして、 Cisco UCS Manager にログインします。




=== Cisco UCS Manager ソフトウェアバージョン 4.0(1b)

このマニュアルでは、 Cisco UCS Manager ソフトウェアバージョン 4.0(1b) を使用することを前提としています。Cisco UCS Manager ソフトウェアおよび Cisco UCS 6324 ファブリックインターコネクトソフトウェアのアップグレードについては、を参照してください  https://www.cisco.com/c/en/us/support/servers-unified-computing/ucs-manager/products-installation-and-configuration-guides-list.html["Cisco UCS Manager インストールおよびアップグレードガイド"^]



=== Cisco UCS Call Home を設定する

Cisco UCS Manager で Call Home を設定することを強く推奨します。Call Home を設定すると、サポートケースの解決が迅速になります。Call Home を設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Admin をクリックします。
. [ すべて ]>[ 通信管理 ]>[ コールホーム ] の順に選択します。
. 状態をオンに変更します。
. 管理設定に従ってすべてのフィールドに入力し、 [ 変更の保存 ] をクリックして [OK] をクリックし、 Call Home の設定を完了します。




=== キーボード、ビデオ、マウスアクセス用の IP アドレスのブロックを追加します

Cisco UCS 環境で帯域内サーバのキーボード、ビデオ、マウス（ KVM ）アクセス用の IP アドレスブロックを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. [Pools] > [root] > [IP Pools] を展開します。
. [IP Pool ext-mgmt] を右クリックし、 [Create Block of IPv4 Addresses] を選択します。
. ブロックの開始 IP アドレス、必要な IP アドレスの数、およびサブネットマスクとゲートウェイの情報を入力します。
+
image:express-direct-attach-aff220-deploy_image7.png["エラー：グラフィックイメージがありません"]

. [OK] をクリックして、ブロックを作成する。
. 確認メッセージで [OK] をクリックします。




=== Cisco UCS を NTP に同期する

Cisco UCS 環境を Nexus スイッチの NTP サーバと同期させるには、次の手順を実行します。

. Cisco UCS Manager で、左側の Admin をクリックします。
. ［ すべて ］ > ［ タイムゾーン管理 ］ を展開します。
. ［ タイムゾーン ］ を選択します。
. [ プロパティ ] ペインで、 [ タイムゾーン ] メニューから適切なタイムゾーンを選択します。
. [Save Changes] をクリックし、 [OK] をクリックします。
. Add NTP Server をクリックします。
. 「 <switch-a-ntp-ip> 」または「 <nexus-a-mgmt-ip>` 」と入力し、 [OK] をクリックします。[OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image8.png["エラー：グラフィックイメージがありません"]

. Add NTP Server をクリックします。
. 「 <switch-b-ntp-ip>`` 」または「 <nexus-B-mgmt-ip>` 」と入力し、 [OK] をクリックします。確認の [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image9.png["エラー：グラフィックイメージがありません"]





=== シャーシ検出ポリシーを編集します

検出ポリシーを設定することで、 Cisco UCS B シリーズシャーシの追加やファブリックエクステンダの追加が簡素化され、 Cisco UCS C シリーズの接続性がさらに向上します。シャーシ検出ポリシーを変更するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [Equipment] をクリックし、 2 番目のリストで [Equipment] を選択します。
. 右側のペインで、 [ ポリシー ] タブを選択します。
. Global Policies （グローバルポリシー）で、シャーシまたはファブリックエクステンダ（ FEX ）とファブリックインターコネクト間でケーブル接続されているアップリンクポートの最小数と一致するように、 Chassis/FEX Discovery Policy （シャーシ /FEX 検出ポリシー）を設定します。
. Link Grouping Preference を Port Channel に設定します。設定する環境に大量のマルチキャストトラフィックが含まれている場合は、 Multicast Hardware Hash （マルチキャストハードウェアハッシュ）設定を Enabled （有効）に設定します。
. [Save Changes] をクリックします。
. [OK] をクリックします。




=== サーバ、アップリンク、およびストレージポートを有効にします

サーバポートとアップリンクポートをイネーブルにするには、次の手順を実行します。

. Cisco UCS Manager のナビゲーションペインで、 Equipment タブを選択します。
. Equipment > Fabric Interconnects > Fabric Interconnect A > Fixed Module の順に展開します。
. [Ethernet ポート ] を展開します。
. Cisco Nexus 31108 スイッチに接続されているポート 1 と 2 を選択し、右クリックして、 [Configure as Uplink Port] を選択します。
. Yes をクリックしてアップリンクポートを確認し、 OK をクリックします。
. ネットアップストレージコントローラに接続されているポート 3 と 4 を選択し、右クリックして Configure as Appliance Port （アプライアンスポートとして設定）を選択します。
. Yes をクリックして、アプライアンスのポートを確認します。
. Configure as Appliance Port （アプライアンスポートとして設定）ウィンドウで、 OK をクリックします。 
. [OK] をクリックして確定します。
. 左側のペインで、 Fabric Interconnect A の Fixed Module を選択します 
. [Ethernet Ports] タブで、 [If Role] カラムにポートが正しく設定されていることを確認します。スケーラビリティポートにポート C シリーズサーバが設定されている場合は、そのサーバをクリックしてポート接続を確認します。
+
image:express-direct-attach-aff220-deploy_image10.png["エラー：グラフィックイメージがありません"]

. Equipment > Fabric Interconnects > Fabric Interconnect B > Fixed Module の順に展開します。
. [Ethernet ポート ] を展開します。
. Cisco Nexus 31108 スイッチに接続されているイーサネットポート 1 および 2 を選択し、右クリックして、 Configure as Uplink Port （アップリンクポートとして設定）を選択します。
. Yes をクリックしてアップリンクポートを確認し、 OK をクリックします。
. ネットアップストレージコントローラに接続されているポート 3 と 4 を選択し、右クリックして Configure as Appliance Port （アプライアンスポートとして設定）を選択します。
. Yes をクリックして、アプライアンスのポートを確認します。
. Configure as Appliance Port （アプライアンスポートとして設定）ウィンドウで、 OK をクリックします。
. [OK] をクリックして確定します。
. 左側のペインで、 Fabric Interconnect B の Fixed Module を選択します 
. [Ethernet Ports] タブで、 [If Role] カラムにポートが正しく設定されていることを確認します。スケーラビリティポートにポート C シリーズサーバが設定されている場合は、そのサーバをクリックしてポート接続を確認します。
+
image:express-direct-attach-aff220-deploy_image11.png["エラー：グラフィックイメージがありません"]





=== Cisco Nexus 31108 スイッチへのアップリンクポートチャネルを作成します

Cisco UCS 環境で必要なポートチャネルを設定するには、次の手順を実行します。

. Cisco UCS Manager で、ナビゲーションペインの [LAN] タブを選択します。
+

NOTE: この手順では、 2 つのポートチャネルが作成されます。 1 つはファブリック A から両方の Cisco Nexus 31108 スイッチへ、もう 1 つはファブリック B から両方の Cisco Nexus 31108 スイッチへです。標準スイッチを使用している場合は、それに応じてこの手順を変更します。ファブリックインターコネクト上で 1 ギガビットイーサネット（ 1GbE ）スイッチおよび GLC-T SFP を使用する場合は、ファブリックインターコネクト内のイーサネットポート 1/1 および 1/2 のインターフェイス速度を 1Gbps に設定する必要があります。

. [LAN] > [LAN Cloud] で、 [Fabric A] ツリーを展開します。
. [ ポートチャネル ] を右クリックします。
. ポートチャネルの作成を選択します。
. ポートチャネルの一意の ID として 13 を入力します。
. ポートチャネルの名前として「 vPC-13-Nexus 」と入力します。
. 次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image12.png["エラー：グラフィックイメージがありません"]

. ポートチャネルに追加する次のポートを選択します。
+
.. スロット ID 1 とポート 1
.. スロット ID 1 とポート 2


. >> をクリックして、ポートチャネルにポートを追加します。
. Finish をクリックして、ポートチャネルを作成します。[OK] をクリックします。
. [ ポートチャネル ] で、新しく作成したポートチャネルを選択します。
+
ポートチャネルの全体的なステータスが up になっている必要があります。

. ナビゲーションペインで、 [LAN] > [LAN Cloud] の下の [Fabric B] ツリーを展開します。
. [ ポートチャネル ] を右クリックします。
. ポートチャネルの作成を選択します。
. ポートチャネルの一意の ID として「 14 」を入力します。
. ポートチャネルの名前として「 vPC-14-Nexus 」と入力します。次へをクリックします。
. ポートチャネルに追加する次のポートを選択します。
+
.. スロット ID 1 とポート 1
.. スロット ID 1 とポート 2


. >> をクリックして、ポートチャネルにポートを追加します。
. Finish をクリックして、ポートチャネルを作成します。[OK] をクリックします。
. [ ポートチャネル ] で、新しく作成したポートチャネルを選択します。
. ポートチャネルの全体的なステータスが up になっている必要があります。




=== 組織の作成（オプション）

組織は、リソースを整理し、 IT 組織内のさまざまなグループへのアクセスを制限することで、コンピューティングリソースのマルチテナンシーを実現するために使用されます。


NOTE: このドキュメントでは組織の使用は想定していませんが、この手順では組織の作成方法について説明します。

Cisco UCS 環境で組織を設定するには、次の手順を実行します。

. Cisco UCS Manager で、ウィンドウ上部のツールバーの [ 新規作成（ New ） ] メニューから、 [ 組織の作成（ Create Organization ） ] を選択します。
. 組織の名前を入力します。
. オプション：組織の概要を入力します。[OK] をクリックします。
. 確認メッセージで [OK] をクリックします。




=== ストレージアプライアンスのポートおよびストレージ VLAN を設定します

ストレージアプライアンスのポートとストレージ VLAN を設定するには、次の手順を実行します。

. Cisco UCS Manager で、 [LAN] タブを選択します。
. アプライアンスクラウドを拡張します。
. アプライアンスクラウドの下の VLAN を右クリックします。
. [Create VLANs] を選択します。
. Infrastructure NFS VLAN の名前として「 nfs-vlan 」と入力します。
. 共通 / グローバルを選択したままにします。
. VLAN ID として「 \<<var_nfs_vlan_id>> 」と入力します。
. [ 共有タイプ ] は [ なし ] のままにします。
+
image:express-direct-attach-aff220-deploy_image13.jpeg["エラー：グラフィックイメージがありません"]

. [OK] をクリックし、もう一度 [OK] をクリックして VLAN を作成します。
. アプライアンスクラウドの下の VLAN を右クリックします。
. [Create VLANs] を選択します。
. Infrastructure iSCSI Fabric A VLAN の名前として「 iSCSI-A-VLAN 」と入力します。
. 共通 / グローバルを選択したままにします。
. VLAN ID として「 \<<var_iscsi-a_vlan_id>> 」と入力します。
. [OK] をクリックし、もう一度 [OK] をクリックして VLAN を作成します。
. アプライアンスクラウドの下の VLAN を右クリックします。
. [Create VLANs] を選択します。
. インフラストラクチャ iSCSI ファブリック B VLAN の名前として「 iSCSI-B-VLAN 」と入力します。
. 共通 / グローバルを選択したままにします。
. VLAN ID として「 \<<var_iscsi-b_vlan_id>> 」と入力します。
. [OK] をクリックし、もう一度 [OK] をクリックして VLAN を作成します。
. アプライアンスクラウドの下の VLAN を右クリックします。
. [Create VLANs] を選択します。
. ネイティブ VLAN の名前として「 Native - VLAN 」と入力します。
. 共通 / グローバルを選択したままにします。
. VLAN ID として「 \<<var_native_vlan_id>> 」と入力します。
. [OK] をクリックし、もう一度 [OK] をクリックして VLAN を作成します。
+
image:express-direct-attach-aff220-deploy_image14.png["エラー：グラフィックイメージがありません"]

. ナビゲーションペインで、 [LAN] > [Policies] の下の [Appliances] を展開し、 [Network Control Policies] を右クリックします。
. Create Network Control Policy を選択します。
. ポリシーに「 Enable_cdp_LLPD 」という名前を付け、 CDP の横にある [ 有効 ] を選択します。
. LLDP の送受信機能を有効にします。
+
image:express-direct-attach-aff220-deploy_image15.png["エラー：グラフィックイメージがありません"]

. [OK] をクリックし、もう一度 [OK] をクリックしてポリシーを作成します。
. ナビゲーションペインの [LAN] > [Appliances Cloud] で、 [Fabric A tree] を展開します。
. [Interfaces] を展開します。
. アプライアンス・インターフェイス 1/3 を選択します。
. [User Label] フィールドに、「 <storage_controller_01_name> ： e0e 」など、ストレージコントローラポートを示す情報を入力します。[ 変更を保存して OK ] をクリックします。
. Enable_CDP Network Control Policy を選択し、 Save Changes and OK を選択します。
. [VLANs] で、 iSCSI-A VLAN 、 NFS VLAN 、およびネイティブ VLAN を選択します。ネイティブ VLAN をネイティブ VLAN として設定します。デフォルトの VLAN 選択をクリアします。
. [ 変更を保存して OK ] をクリックします。
+
image:express-direct-attach-aff220-deploy_image16.png["エラー：グラフィックイメージがありません"]

. [Fabric A] の下にある [Appliance Interface] 1/4 を選択します
. [User Label] フィールドに、「 <storage_controller_02_name> ： e0e 」など、ストレージコントローラポートを示す情報を入力します。[ 変更を保存して OK ] をクリックします。
. Enable_CDP Network Control Policy を選択し、 Save Changes and OK を選択します。
. [VLANs] で、 iSCSI-A VLAN 、 NFS VLAN 、およびネイティブ VLAN を選択します。
. ネイティブ VLAN をネイティブ VLAN として設定します。 
. デフォルトの VLAN 選択をクリアします。
. [ 変更を保存して OK ] をクリックします。
. ナビゲーションペインの [LAN] > [Appliances Cloud] で、 [Fabric B] ツリーを展開します。
. [Interfaces] を展開します。
. アプライアンス・インターフェイス 1/3 を選択します。
. [User Label] フィールドに、「 <storage_controller_01_name> ： e0f 」など、ストレージコントローラポートを示す情報を入力します。[ 変更を保存して OK ] をクリックします。
. Enable_CDP Network Control Policy を選択し、 Save Changes and OK を選択します。
. [VLANs] で、 [iSCSI-B-VLAN] 、 [NFS VLAN] 、および [ ネイティブ VLAN] を選択します。ネイティブ VLAN をネイティブ VLAN として設定します。デフォルト VLAN の選択を解除します。
+
image:express-direct-attach-aff220-deploy_image17.png["エラー：グラフィックイメージがありません"]

. [ 変更を保存して OK ] をクリックします。
. [Fabric B] の下にある [Appliance Interface] 1/4 を選択します
. [User Label] フィールドに、「 <storage_controller_02_name> ： e0f 」など、ストレージコントローラポートを示す情報を入力します。[ 変更を保存して OK ] をクリックします。
. Enable_CDP Network Control Policy を選択し、 Save Changes and OK を選択します。
. [VLANs] で、 [iSCSI-B-VLAN] 、 [NFS VLAN] 、および [ ネイティブ VLAN] を選択します。ネイティブ VLAN をネイティブ VLAN として設定します。デフォルト VLAN の選択を解除します。
. [ 変更を保存して OK ] をクリックします。




=== Cisco UCS ファブリックでジャンボフレームを設定します

Cisco UCS ファブリックでジャンボフレームを設定して QoS を有効にするには、次の手順を実行します。

. Cisco UCS Manager のナビゲーションペインで、 [LAN] タブをクリックします。
. [LAN] > [LAN Cloud] > [QoS System Class] の順に選択します。
. 右側のペインで、 [ 全般 ] タブをクリックします。
. [ ベストエフォート ] 行で、 [MTU] 列の下のボックスに 9216 と入力します。
+
image:express-direct-attach-aff220-deploy_image18.png["エラー：グラフィックイメージがありません"]

. [Save Changes] をクリックします。
. [OK] をクリックします。




=== Cisco UCS シャーシを確認します

すべての Cisco UCS シャーシを確認するには、次の手順を実行します。

. Cisco UCS Manager で、 [Equipment] タブを選択し、右側の [Equipment] タブを展開します。
. 機器 > シャーシを展開します。
. シャーシ 1 のアクションでシャーシの確認を選択します。
. [OK] をクリックし、 [OK] をクリックしてシャーシの確認を完了します。
. [ 閉じる ] をクリックして、 [ プロパティ ] ウィンドウを閉じます。




=== Cisco UCS 4.0(1b) ファームウェアイメージをロードします

Cisco UCS Manager ソフトウェアと Cisco UCS Fabric Interconnect ソフトウェアをバージョン 4.0(1b) にアップグレードするには、を参照してください https://www.cisco.com/en/US/products/ps10281/prod_installation_guides_list.html["Cisco UCS Manager インストールおよびアップグレードガイド"^]。



=== ホストファームウェアパッケージを作成する

ファームウェア管理ポリシーを使用すると、管理者は特定のサーバ設定に対応するパッケージを選択できます。これらのポリシーには、多くの場合、アダプタ、 BIOS 、ボードコントローラ、 FC アダプタ、ホストバスアダプタ（ HBA ）オプション ROM 、ストレージコントローラプロパティのパッケージが含まれています。

Cisco UCS 環境で特定のサーバ設定のファームウェア管理ポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. ホストファームウェアパッケージを展開します。
. デフォルトを選択します。
. アクションペインで、パッケージバージョンの変更を選択します。
. 両方のブレードパッケージのバージョン 4.0(1b) を選択します。
+
image:express-direct-attach-aff220-deploy_image19.png["エラー：グラフィックイメージがありません"]

. [OK] をクリックし、もう一度 [OK] をクリックして、ホストファームウェアパッケージを変更します。




=== MAC アドレスプールを作成します

Cisco UCS 環境に必要な MAC アドレスプールを設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. プール／ルートを選択します。
+
この手順では、スイッチングファブリックごとに 1 つずつ、 2 つの MAC アドレスプールが作成されます。

. ルート組織の下にある [MAC Pools] を右クリックします。
. MAC アドレスプールを作成するには、 Create MAC Pool （ MAC プールの作成）を選択します。
. MAC プールの名前として「 MAC-Pool-A 」と入力します。
. オプション： MAC プールの概要を入力します。
. 割り当て順序（ Assignment Order ）のオプションとして順次（ Sequential ）を選択します。次へをクリックします。
. 追加をクリックします。
. 開始 MAC アドレスを指定します。
+

NOTE: FlexPod 解決策では、開始 MAC アドレスの最後のオクテットに 0a を配置して、すべての MAC アドレスをファブリック A アドレスとして識別することを推奨します。この例では、最初の MAC アドレスとして 00 ： 25 ： B5 ： 32 ： 0a:00 を与える Cisco UCS ドメイン番号情報も組み込みました。

. 使用可能なブレードまたはサーバリソースをサポートするのに十分な MAC アドレスプールのサイズを指定します。[OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image20.png["エラー：グラフィックイメージがありません"]

. 完了をクリックします。
. 確認メッセージが表示されたら、 [OK] をクリックします。
. ルート組織の下にある [MAC Pools] を右クリックします。
. MAC アドレスプールを作成するには、 Create MAC Pool （ MAC プールの作成）を選択します。
. MAC プールの名前として「 MAC-Pool-B 」と入力します。
. オプション： MAC プールの概要を入力します。
. 割り当て順序（ Assignment Order ）のオプションとして順次（ Sequential ）を選択します。次へをクリックします。
. 追加をクリックします。
. 開始 MAC アドレスを指定します。
+

NOTE: FlexPod 解決策の場合、このプール内のすべての MAC アドレスをファブリック B アドレスとして識別するために、開始 MAC アドレスの最後のオクテットの隣に 0B を配置することを推奨します。この例では、最初の MAC アドレスとして 00 ： 25 ： B5 ： 32 ： 0B ： 00 を与える Cisco UCS ドメイン番号情報も組み込みました。

. 使用可能なブレードまたはサーバリソースをサポートするのに十分な MAC アドレスプールのサイズを指定します。[OK] をクリックします。
. 完了をクリックします。
. 確認メッセージが表示されたら、 [OK] をクリックします。




=== iSCSI IQN プールを作成します

Cisco UCS 環境に必要な IQN プールを設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [SAN] をクリックします。
. プール／ルートを選択します。
. IQN プールを右クリックします。
. IQN サフィックスプールの作成を選択して IQN プールを作成します。
. IQN プールの名前として「 IQN -Pool 」と入力します。
. オプション： IQN プールの概要を入力します。
. プレフィックスとして「 iqn.1992-08.com.cisco` 」と入力します。
. [ 割り当て順序 ] で [ 順次 ] を選択します。次へをクリックします。
. 追加をクリックします。
. サフィックスに「 UCS-host」 と入力します。
+

NOTE: 複数の Cisco UCS ドメインを使用している場合は、さらに具体的な IQN サフィックスを使用する必要があります。

. [From] フィールドに 1 を入力します。
. 使用可能なサーバリソースを十分にサポートできる IQN ブロックのサイズを指定してください。[OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image21.png["エラー：グラフィックイメージがありません"]

. 完了をクリックします。




=== iSCSI イニシエータの IP アドレスプールを作成します

Cisco UCS 環境に必要な IP プール iSCSI ブートを設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. プール／ルートを選択します。
. [IP Pools] を右クリックします。
. Create IP Pool を選択します。
. IP プール名として「 iSCSI-IP-Pool-A 」と入力します。
. オプション： IP プールの概要を入力します。
. 割り当て順序の [ 順次 ] を選択します。次へをクリックします。
. Add をクリックして IP アドレスのブロックを追加します。
. [From] フィールドに、 iSCSI IP アドレスとして割り当てる範囲の先頭を入力します。
. サーバに対応できる十分なアドレスにサイズを設定してください。[OK] をクリックします。
. 次へをクリックします。
. 完了をクリックします。
. [IP Pools] を右クリックします。
. Create IP Pool を選択します。
. IP プール名として「 iSCSI-IP-Pool-B 」と入力します。
. オプション： IP プールの概要を入力します。
. 割り当て順序の [ 順次 ] を選択します。次へをクリックします。
. Add をクリックして IP アドレスのブロックを追加します。
. [From] フィールドに、 iSCSI IP アドレスとして割り当てる範囲の先頭を入力します。
. サーバに対応できる十分なアドレスにサイズを設定してください。[OK] をクリックします。
. 次へをクリックします。
. 完了をクリックします。




=== UUID サフィックスプールを作成します

Cisco UCS 環境に必要な Universally Unique Identifier （ UUID ）サフィックスプールを設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. プール／ルートを選択します。
. [UUID Suffix Pools] を右クリックします。
. [Create UUID Suffix Pool] を選択します。
. UUID サフィックスプールの名前として「 UUID - プール」と入力します。
. オプション： UUID サフィックスプールの概要を入力します。
. 接頭部は派生オプションのままにします。
. 割り当て順序（ Assignment Order ）に順次（ Sequential ）を選択し
. 次へをクリックします。
. Add をクリックして UUID のブロックを追加します。
. デフォルト設定の [From] フィールドをそのまま使用します。
. 使用可能なブレードまたはサーバリソースをサポートするのに十分な UUID ブロックのサイズを指定します。[OK] をクリックします。
. 完了をクリックします。
. [OK] をクリックします。




=== サーバプールを作成します

Cisco UCS 環境に必要なサーバプールを設定するには、次の手順を実行します。


NOTE: 環境で必要とされる細分性を実現するために、固有のサーバプールを作成することを検討してください。

. Cisco UCS Manager で、左側の Servers をクリックします。
. プール／ルートを選択します。
. [ サーバープール ] を右クリックします。
. Create Server Pool を選択します。
. サーバ・プールの名前として「 Infra-Pool 」と入力します。
. オプション：サーバプールの概要を入力します。次へをクリックします。
. VMware 管理クラスタに使用するサーバを 2 つ以上選択し '>> をクリックして Infra-Pool' Server プールに追加します
. 完了をクリックします。
. [OK] をクリックします。




=== Cisco Discovery Protocol と Link Layer Discovery Protocol のネットワーク制御ポリシーを作成します

Cisco Discovery Protocol （ CDP ）および Link Layer Discovery Protocol （ LLDP ）のネットワーク制御ポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [ ネットワーク制御ポリシー ] を右クリックします。
. Create Network Control Policy を選択します。
. Enable-CDP-LLDP ポリシー名を入力します。
. CDP の場合は、 Enabled オプションを選択します。
. LLDP の場合は、下にスクロールして、送信と受信の両方で有効を選択します。
. [OK] をクリックして、ネットワーク制御ポリシーを作成します。[OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image22.png["エラー：グラフィックイメージがありません"]





=== 電源制御ポリシーを作成します

Cisco UCS 環境の電源制御ポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers タブをクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [ 電源制御ポリシー ] を右クリックします。
. 電源制御ポリシーの作成を選択します。
. 電源制御ポリシー名として No-Power-Cap と入力します。
. 電力上限設定を [No Cap]( キャップなし ) に変更します
. [OK] をクリックして、電源制御ポリシーを作成します。[OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image23.png["エラー：グラフィックイメージがありません"]





=== サーバプール認定ポリシーの作成（オプション）

Cisco UCS 環境のオプションのサーバプール認定ポリシーを作成するには、次の手順を実行します。


NOTE: この例では、 Intel E2660 v4 Xeon Broadwell プロセッサを搭載した Cisco UCS B シリーズサーバ用のポリシーを作成します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [ サーバプールポリシーの条件 ] を選択します。
. Create Server Pool Policy Qualification （サーバプールポリシーの作成条件）または Add （追加）を
. ポリシーにインテルという名前を付けます。
. Create CPU/ Cores Qualifications] を選択します。
. プロセッサ / アーキテクチャに Xeon を選択します。
. プロセス ID （ PID ）として「 <UCS-CPU-PID>` 」と入力します。
. [OK] をクリックして、 CPU/ コアの資格情報を作成します。
. [OK] をクリックしてポリシーを作成し、 [OK] をクリックして確認します。
+
image:express-direct-attach-aff220-deploy_image24.png["エラー：グラフィックイメージがありません"]





=== サーバ BIOS ポリシーを作成します

Cisco UCS 環境のサーバ BIOS ポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. BIOS Policies （ BIOS ポリシー）を右クリックします。
. [Create BIOS Policy] を選択します。
. BIOS ポリシー名として「 VM-Host 」と入力します。
. Quiet Boot 設定を disabled に変更します。
. 一貫したデバイス名を有効に変更します。
+
image:express-direct-attach-aff220-deploy_image25.png["エラー：グラフィックイメージがありません"]

. [ プロセッサ ] タブを選択し、次のパラメータを設定します。
+
** プロセッサ C の状態：無効
** プロセッサ C1E ：無効
** プロセッサ C3 レポート : 無効
** プロセッサ C7 レポート：無効
+
image:express-direct-attach-aff220-deploy_image26.png["エラー：グラフィックイメージがありません"]



. 残りのプロセッサオプションまで下にスクロールして、次のパラメータを設定します。
+
** エネルギー性能：パフォーマンス
** 周波数下限のオーバーライド：有効
** DRAM Clock Throttling ：パフォーマンス
+
image:express-direct-attach-aff220-deploy_image27.png["エラー：グラフィックイメージがありません"]



. [RAS メモリ ] をクリックして、次のパラメータを設定します。
+
** LV DDR モード：パフォーマンスモード
+
image:express-direct-attach-aff220-deploy_image28.png["エラー：グラフィックイメージがありません"]



. Finish をクリックして、 BIOS ポリシーを作成します。
. [OK] をクリックします。




=== デフォルトのメンテナンスポリシーを更新する

デフォルトのメンテナンスポリシーを更新するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [ メンテナンスポリシー ]>[ デフォルト ] を選択します。
. Reboot Policy を User Ack に変更します
. [ 次のブート時 ] を選択して、メンテナンス時間をサーバー管理者に委任します。
+
image:express-direct-attach-aff220-deploy_image29.png["エラー：グラフィックイメージがありません"]

. [Save Changes] をクリックします。
. [OK] をクリックして変更を確定します。




=== vNIC テンプレートを作成します

Cisco UCS 環境用に複数の仮想ネットワークインターフェイスカード（ vNIC ）テンプレートを作成するには、この項で説明する手順を実行します。


NOTE: 合計 4 つの vNIC テンプレートが作成されます。



==== インフラストラクチャ vNIC を作成します

インフラストラクチャ vNIC を作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [vNIC Templates] を右クリックします。
. [Create vNIC Template] を選択します。
. vNIC テンプレート名として「 ite -XX-vnic_a 」と入力します。
. [ テンプレートタイプ ] として [ 更新テンプレート ] を選択します。
. [Fabric ID] に [Fabric A] を選択します
. [Enable Failover] オプションが選択されていないことを確認します。
. [ 冗長性タイプ ] の [ プライマリテンプレート ] を選択します。
. ピア冗長性テンプレートを「 <not set> 」のままにします。
. [ ターゲット ] で、 [ アダプタ ] オプションのみが選択されていることを確認します。
. ネイティブ VLAN として 'Native - VLAN' を設定します
. CDN ソースの vNIC 名を選択します。
. MTU の場合は 9000 と入力します。
. [Permitted VLANs] で、 [Native - VLAN] 、 [Site-XX-IB-MGMT] 、 [Site-XX-NFS] 、 [Site-XX-VM-Traffic] を選択します。 および Site-XX-MvMotion複数選択するには、 Ctrl キーを使用します。
. 選択をクリックします。これらの VLAN が Selected VLANs の下に表示されます。
. [MAC Pool] リストで、 [`M AC_Pool_A`] を選択します。
. [ ネットワーク制御ポリシー ] リストで、 [ プール A ] を選択します
. [ ネットワーク制御ポリシー ] リストで、 [ 有効 - CDP-LLDP ] を選択します。
. [OK] をクリックして、 vNIC テンプレートを作成します。
. [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image30.png["エラー：グラフィックイメージがありません"]



セカンダリ冗長テンプレート Infra-B を作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [vNIC Templates] を右クリックします。
. [Create vNIC Template] を選択します。
. vNIC テンプレート名として「 ite-XX-vnic_B 」と入力します。
. [ テンプレートタイプ ] として [ 更新テンプレート ] を選択します。
. [Fabric ID] に [Fabric B] を選択します
. [Enable Failover] オプションを選択します。
+

NOTE: フェールオーバーを選択することは、ハードウェアレベルでリンクのフェールオーバー時間を改善し、仮想スイッチで検出されない NIC 障害の可能性を防ぐための重要なステップです。

. [ 冗長性タイプ ] の [ プライマリテンプレート ] を選択します。
. ピア冗長性テンプレートは 'vNIC_Template_A' のままにします
. [ ターゲット ] で、 [ アダプタ ] オプションのみが選択されていることを確認します。
. ネイティブ VLAN として 'Native - VLAN' を設定します
. CDN ソースの vNIC 名を選択します。
. MTU には '9000' と入力します
. [Permitted VLANs] で、 [Native - VLAN] 、 [Site-XX-IB-MGMT] 、 [Site-XX-NFS] 、 [Site-XX-VM-Traffic] を選択します。 および Site-XX-MvMotion複数選択するには、 Ctrl キーを使用します。
. 選択をクリックします。これらの VLAN が Selected VLANs の下に表示されます。
. [MAC Pool] リストで、 [`M AC_Pool_b] を選択します。
. [Network Control Policy] リストで、 [Pool-B ] を選択します
. [ ネットワーク制御ポリシー ] リストで、 [ 有効 - CDP-LLDP ] を選択します。 
. [OK] をクリックして、 vNIC テンプレートを作成します。
. [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image31.png["エラー：グラフィックイメージがありません"]





==== iSCSI vNIC を作成します

iSCSI vNIC を作成するには、次の手順を実行します。

. 左側の [LAN] を選択します。
. [ ポリシー ]>[ ルート ] を選択します。
. [vNIC Templates] を右クリックします。
. [Create vNIC Template] を選択します。 
. vNIC テンプレート名として「 'Site-01-iSCSI_A' 」を入力します。
. [Fabric A] を選択します[Enable Failover] オプションは選択しないでください。 
. 冗長性タイプを冗長性なしに設定したままにします。
. [ ターゲット ] で、 [ アダプタ ] オプションのみが選択されていることを確認します。
. [ テンプレートタイプ ] で [ テンプレートの更新 ] を選択します。
. [VLANs] で、 [Site-01-iSCSI_A_VLAN] だけを選択します。
. [Site-01-iSCSI_A_VLAN] をネイティブ VLAN として選択します。
. CDN ソースに対して vNIC 名を設定したままにします。 
. MTU の下に 9000 と入力します。 
. MAC Pool リストから MAC-Pool-A を選択します
. Network Control Policy リストから、 Enable-CDP-LLDP を選択します。
. [OK] をクリックして、 vNIC テンプレートの作成を完了します。
. [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image32.png["エラー：グラフィックイメージがありません"]

. 左側の [LAN] を選択します。
. [ ポリシー ]>[ ルート ] を選択します。
. [vNIC Templates] を右クリックします。
. [Create vNIC Template] を選択します。
. vNIC テンプレート名として「 Site-01-iSCSI_B 」を入力します。
. ファブリック B を選択します[Enable Failover] オプションは選択しないでください。
. 冗長性タイプを冗長性なしに設定したままにします。
. [ ターゲット ] で、 [ アダプタ ] オプションのみが選択されていることを確認します。
. [ テンプレートタイプ ] で [ テンプレートの更新 ] を選択します。
. [VLANs] で、 [`s it-01-iscsi_B_VLAN] のみを選択します。
. ネイティブ VLAN として [`s it-01-iSCSI_B_VLAN] を選択します。
. CDN ソースに対して vNIC 名を設定したままにします。
. MTU の下に 9000 と入力します。
. [MAC Pool] リストから、 [`M AC-Pool-B] を選択します。 
. [ ネットワーク制御ポリシー ] リストから、 [ 有効 - CDP-LLDP-M] を選択します。
. [OK] をクリックして、 vNIC テンプレートの作成を完了します。
. [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image33.png["エラー：グラフィックイメージがありません"]





=== iSCSI ブート用の LAN 接続ポリシーを作成します

この手順環境は、 2 つの iSCSI LIF がクラスタノード 1 （「 iscsi_dlif01a 」および「 iscsi_dlif01b 」）にあり、 2 つの iSCSI LIF がクラスタノード 2 （「 iscsi_dlif02a 」および「 iscsi_dlif02b 」）にある Cisco UCS 環境です。また、 A LIF がファブリック A （ Cisco UCS 6324 A ）に接続され、 B LIF がファブリック B （ Cisco UCS 6324 B ）に接続されていると想定しています。

必要なインフラストラクチャ LAN 接続ポリシーを設定するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [LAN] をクリックします。
. [LAN] > [Policies] > [root] を選択します。
. [LAN 接続ポリシー ] を右クリックします。
. [Create LAN Connectivity Policy] を選択します。
. ポリシー名として「 ite-XX-fFabric-a」 と入力します。
. vNIC を追加するには、上部の Add オプションをクリックします。
. [Create vNIC] ダイアログボックスで、 vNIC の名前として「 S`ite-01-vNIC-A` 」と入力します。
. [Use vNIC Template] オプションを選択します。
. [vNIC Template] リストで、 [vNIC_Template_A] を選択します。
. [Adapter Policy] ドロップダウンリストから [VMware] を選択します。
. [OK] をクリックして、この vNIC をポリシーに追加します。
+
image:express-direct-attach-aff220-deploy_image34.png["エラー：グラフィックイメージがありません"]

. vNIC を追加するには、上部の Add オプションをクリックします。
. [Create vNIC] ダイアログボックスで、 vNIC の名前として「 S`it-01-vNIC-B` 」と入力します。
. [Use vNIC Template] オプションを選択します。
. [vNIC Template] リストで、 [vNIC_Template_B] を選択します。
. [Adapter Policy] ドロップダウンリストから [VMware] を選択します。
. [OK] をクリックして、この vNIC をポリシーに追加します。
. vNIC を追加するには、上部の Add オプションをクリックします。
. [Create vNIC] ダイアログボックスで、 vNIC の名前として「 `sit-01-iscsi-A` 」と入力します。
. [Use vNIC Template] オプションを選択します。
. [vNIC Template] リストで、 [`site-01-iSCSI-A] を選択します。
. [Adapter Policy] ドロップダウンリストから [VMware] を選択します。
. [OK] をクリックして、この vNIC をポリシーに追加します。
. vNIC を追加するには、上部の Add オプションをクリックします。
. [Create vNIC] ダイアログボックスで、 vNIC の名前として「 S`ite-01-iSCSI-B` 」と入力します。
. [Use vNIC Template] オプションを選択します。
. [vNIC Template] リストで、 [`Site-01-iSCSI-B] を選択します。
. [Adapter Policy] ドロップダウンリストから [VMware] を選択します。
. [OK] をクリックして、この vNIC をポリシーに追加します。
. Add iSCSI vNICs オプションを展開します。
. [Add iSCSI vNICs] スペースの下側の [Add] オプションをクリックして、 iSCSI vNIC を追加します。
. [Create iSCSI vNIC] ダイアログボックスで、 vNIC の名前として「 `Site-01-iSCSI-A 」を入力します。
. [Overlay vNIC] を [`s ite -01-iSCSI-A] として選択します。
. [iSCSI Adapter Policy] オプションは [Not Set] のままにします。
. VLAN を「 ite-01-iSCSI-Site-A 」（ネイティブ）として選択します。
. MAC アドレスの割り当てとして、 None （なし）（デフォルトで使用）を選択します。
. [OK] をクリックして、 iSCSI vNIC をポリシーに追加します。
+
image:express-direct-attach-aff220-deploy_image35.png["エラー：グラフィックイメージがありません"]

. [Add iSCSI vNICs] スペースの下側の [Add] オプションをクリックして、 iSCSI vNIC を追加します。
. [Create iSCSI vNIC] ダイアログボックスで、 vNIC の名前として「 `Site-01-iSCSI-B 」を入力します。
. Overlay vNIC を Site-01-iSCSI-B として選択します
. [iSCSI Adapter Policy] オプションは [Not Set] のままにします。
. VLAN を「 ite-01-iSCSI-Site-B 」（ネイティブ）として選択します。
. MAC アドレスの割り当てとして、 [ なし ] （デフォルトで使用）を選択します。
. [OK] をクリックして、 iSCSI vNIC をポリシーに追加します。
. [Save Changes] をクリックします。
+
image:express-direct-attach-aff220-deploy_image36.png["エラー：グラフィックイメージがありません"]





==== VMware ESXi 6.7U1 インストールブート用の vMedia ポリシーを作成します

NetApp Data ONTAP のセットアップ手順では、 NetApp Data ONTAP と VMware ソフトウェアのホストに使用する HTTP Web サーバが必要です。ここで作成される vMedia ポリシーは、 VMware ESXi 6 をマッピングします。ESXi のインストールをブートするために Cisco UCS サーバに接続された 7U1 ISO 。このポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の [Servers] を選択します。
. [ ポリシー ]>[ ルート ] を選択します。
. [vMedia Policies] を選択します。
. [ 追加 ] をクリックして、新しい vMedia ポリシーを作成します。
. ポリシーに「 esxi- 6.7U1-HTTP 」という名前を付けます。
. 概要フィールドに ESXi 6.7U1 用のマウント ISO と入力します。
. [ マウント失敗時の再試行 ] で [ はい ] を選択します
. 追加をクリックします。
. マウントに esxi- 6.7U1-HTTP という名前を付けます。
. CDD デバイスタイプを選択します。
. HTTP プロトコルを選択します。
. Web サーバの IP アドレスを入力します。
+

NOTE: DNS サーバの IP は KVM IP に入力されていなかったため、ホスト名ではなく Web サーバの IP を入力する必要があります。

. リモートファイル名として「 VMware-VMvator-Installer-6.7.0.update01-10302608.x86_64 .iso 」と入力します。
+
この VMware ESXi 6.7U1 ISO は、からダウンロードできます https://my.vmware.com/group/vmware/details?downloadGroup=ESXI650A&productId=614["VMware のダウンロード"^]。

. [ リモートパス ] フィールドに ISO ファイルへの Web サーバパスを入力します。
. [OK] をクリックして、 vMedia マウントを作成します。
. [OK] をクリックし、もう一度 [OK] をクリックして、 vMedia ポリシーの作成を完了します。
+
Cisco UCS 環境に追加された新しいサーバでは、 vMedia サービスプロファイルテンプレートを使用して ESXi ホストをインストールできます。SAN でマウントされたディスクが空の場合、初回ブート時に ESXi インストーラでホストがブートします。ESXi のインストール後、起動ディスクがアクセス可能である限り、 vMedia は参照されません。

+
image:express-direct-attach-aff220-deploy_image37.png["エラー：グラフィックイメージがありません"]





=== iSCSI ブートポリシーを作成します

ここで説明する環境の手順は、 2 つの iSCSI 論理インターフェイス（ LIF ）がクラスタノード 1 （「 iscsi_dlif01a 」および「 iscsi_dlif01b 」）にあり、 2 つの iSCSI LIF がクラスタノード 2 （「 iscsi_dlif02a 」および「 iscsi_dlif02b 」）にある Cisco UCS 環境です。また、 A LIF がファブリック A （ Cisco UCS ファブリックインターコネクト A ）に接続され、 B LIF がファブリック B （ Cisco UCS ファブリックインターコネクト B ）に接続されていることも前提となります。


NOTE: この手順には、 1 つのブートポリシーが設定されています。このポリシーでは ' プライマリ・ターゲットを iSCSI lif01a に設定します

Cisco UCS 環境のブートポリシーを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ ポリシー ]>[ ルート ] を選択します。
. [Boot Policies] を右クリックします。
. Create Boot Policy を選択します。
. ブートポリシーの名前として「 'Site-01-Fabric-a' 」を入力します。
. オプション：ブートポリシーの概要を入力します。
. Boot Order Change オプションを選択解除したまま再起動します。
. 起動モードはレガシーです。
. [ ローカルデバイス ] ドロップダウンメニューを展開し、 [ リモート CD/DVD の追加 ] を選択します。
. [iSCSI vNICs] ドロップダウンメニューを展開し、 [Add iSCSI Boot] を選択します。
. [Add iSCSI Boot] ダイアログボックスに「 'Site-01-iSCSI-A 」と入力します。[OK] をクリックします。
. Add iSCSI Boot を選択します。
. [Add iSCSI Boot] ダイアログボックスに「 'Site-01-iSCSI-B' 」と入力します。[OK] をクリックします。
. [OK] をクリックして、ポリシーを作成します。
+
image:express-direct-attach-aff220-deploy_image38.png["エラー：グラフィックイメージがありません"]





=== サービスプロファイルテンプレートを作成します

この手順では、ファブリック A ブート用にインフラ ESXi ホスト用のサービスプロファイルテンプレートが 1 つ作成されます。

サービスプロファイルテンプレートを作成するには、次の手順を実行します。

. Cisco UCS Manager で、左側の Servers をクリックします。
. [ サービスプロファイルテンプレート ]>[ ルート ] を選択します。
. ルートを右クリックします。
. [ サービスプロファイルテンプレートの作成 ] を選択して、 [ サービスプロファイルテンプレートの作成 ] ウィザードを開きます。
. サービス・プロファイル・テンプレートの名前として 'VM-Host-Infra-iSCSI-A を入力しますこのサービスプロファイルテンプレートは、ファブリック A のストレージノード 1 からブートするように設定されています
. [ テンプレートの更新 ] オプションを選択します。
. [UUID] で、 [UUID_Pool] を UUID プールとして選択します。次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image39.png["エラー：グラフィックイメージがありません"]





==== ストレージプロビジョニングを設定する

ストレージプロビジョニングを設定するには、次の手順を実行します。

. 物理ディスクを持たないサーバーがある場合は、ローカルディスク設定ポリシーをクリックし、 SAN ブートローカルストレージポリシーを選択します。それ以外の場合は、デフォルトのローカルストレージポリシーを選択します。
. 次へをクリックします。




==== ネットワークオプションを設定します

ネットワークオプションを設定するには、次の手順を実行します。

. ダイナミック vNIC 接続ポリシーのデフォルト設定を保持します。
. Use Connectivity Policy オプションを選択して、 LAN 接続を設定します。
. [LAN Connectivity Policy] ドロップダウンメニューから [iSCSI-Boot] を選択します。
. [ イニシエータ名の割り当て ] で [IQN_Pool] を選択します次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image40.png["エラー：グラフィックイメージがありません"]





==== SAN 接続を設定

SAN 接続を設定するには、次の手順を実行します。

. vHBA の場合は、 SAN 接続を構成する方法を選択します。オプション
. 次へをクリックします。




==== ゾーニングを設定します

ゾーニングを設定するには '[ 次へ ] をクリックします



==== vNII/HBA の配置を設定します

vNII/HBA の配置を設定するには、次の手順を実行します。

. 配置を選択 (Select Placement) ドロップダウンリストから ' 配置ポリシーをシステムが配置を実行できるようにします
. 次へをクリックします。




==== vMedia ポリシーを設定します

vMedia ポリシーを設定するには、次の手順を実行します。

. vMedia ポリシーは選択しないでください。
. 次へをクリックします。




==== サーバのブート順序を設定します

サーバのブート順序を設定するには、次の手順を実行します。

. ブート・ポリシーに [Boot - Fabric-a] を選択します
+
image:express-direct-attach-aff220-deploy_image41.png["エラー：グラフィックイメージがありません"]

. Boor 注文で、「ライト -01-iSCSI-A 」を選択します。
. iSCSI 起動パラメータの設定をクリックします。
. iSCSI ブートパラメータの設定ダイアログボックスで、環境に適した認証プロファイルを個別に作成していない限り、認証プロファイルオプションを Not Set のままにします。
. [ イニシエータ名の割り当て ] ダイアログボックスは、前の手順で定義した単一のサービスプロファイルのイニシエータ名を使用するように設定されていないままにします。
. 「 iSCSI_IP_Pool_A 」をイニシエータ IP アドレス・ポリシーとして設定します。
. iSCSI Static Target Interface オプションを選択します。
. 追加をクリックします。
. iSCSI ターゲット名を入力します。Infra-SVM の iSCSI ターゲット名を取得するには ' ストレージ・クラスタ管理インタフェースにログインして 'iSCSI show コマンドを実行します
+
image:express-direct-attach-aff220-deploy_image42.png["エラー：グラフィックイメージがありません"]

. IPv4 Address フィールドに「 iSCSI_LIF_02a 」の IP アドレスを入力します。
+
image:express-direct-attach-aff220-deploy_image43.png["エラー：グラフィックイメージがありません"]

. OK をクリックして、 iSCSI 静的ターゲットを追加します。
. 追加をクリックします。
. iSCSI ターゲット名を入力します。
. IPv4 Address フィールドに 'iSCSI_LIF_01a' の IP アドレスを入力します
+
image:express-direct-attach-aff220-deploy_image44.png["エラー：グラフィックイメージがありません"]

. OK をクリックして、 iSCSI 静的ターゲットを追加します。
+
image:express-direct-attach-aff220-deploy_image45.png["エラー：グラフィックイメージがありません"]

+

NOTE: ストレージノード 02 の IP を最初に、ストレージノード 01 の IP を 2 番目にして、ターゲット IP を入力しました。これは、ブート LUN がノード 01 にあることを前提としています。この手順で順序が使用されている場合、ホストはノード 01 へのパスを使用してブートします。

. 起動順序で、 [iSCSI-B-vNIC] を選択します。
. iSCSI 起動パラメータの設定をクリックします。
. iSCSI ブートパラメータの設定ダイアログボックスで、環境に適した認証プロファイルを個別に作成していない限り、認証プロファイルオプションは Not Set のままにします。
. [ イニシエータ名の割り当て ] ダイアログボックスは、前の手順で定義した単一のサービスプロファイルのイニシエータ名を使用するように設定されていないままにします。
. イニシエータの IP アドレス・ポリシーとして 'iSCSI_IP_Pool_B' を設定します
. iSCSI Static Target Interface オプションを選択します。
. 追加をクリックします。
. iSCSI ターゲット名を入力します。Infra-SVM の iSCSI ターゲット名を取得するには ' ストレージ・クラスタ管理インタフェースにログインして 'iSCSI show コマンドを実行します
+
image:express-direct-attach-aff220-deploy_image42.png["エラー：グラフィックイメージがありません"]

. IPv4 Address フィールドに 'iSCSI_LIF_02b' の IP アドレスを入力します
+
image:express-direct-attach-aff220-deploy_image46.png["エラー：グラフィックイメージがありません"]

. OK をクリックして、 iSCSI 静的ターゲットを追加します。
. 追加をクリックします。
. iSCSI ターゲット名を入力します。
. IPv4 Address フィールドに 'iSCSI_LIF_01b' の IP アドレスを入力します
+
image:express-direct-attach-aff220-deploy_image47.png["エラー：グラフィックイメージがありません"]

. OK をクリックして、 iSCSI 静的ターゲットを追加します。
+
image:express-direct-attach-aff220-deploy_image48.png["エラー：グラフィックイメージがありません"]

. 次へをクリックします。




==== メンテナンスポリシーを設定する

メンテナンスポリシーを設定するには、次の手順を実行します。

. メンテナンスポリシーをデフォルトに変更します。
+
image:express-direct-attach-aff220-deploy_image49.png["エラー：グラフィックイメージがありません"]

. 次へをクリックします。




==== サーバの割り当てを設定します

サーバ割り当てを設定するには、次の手順を実行します。

. ［ プールの割り当て ］ リストで ［ インフラプール ］ を選択します。
. プロファイルがサーバーに関連付けられている場合に適用する電源状態として、 ［ Down ］ を選択します。
. ページ下部のファームウェア管理を展開し、デフォルトポリシーを選択します。
+
image:express-direct-attach-aff220-deploy_image50.png["エラー：グラフィックイメージがありません"]

. 次へをクリックします。




==== 運用ポリシーを設定

運用ポリシーを設定するには、次の手順を実行します。

. BIOS Policy ドロップダウンリストから VM-Host を選択します。
. Power Control Policy Configuration （電源制御ポリシーの設定）を展開し、 Power Control Policy （電源制御ポリシー）ドロップダウンリストから No-Power-Cap （電源なし - 電力上限）を選択します。
+
image:express-direct-attach-aff220-deploy_image51.png["エラー：グラフィックイメージがありません"]

. [ 完了 ] をクリックして、サービスプロファイルテンプレートを作成します。
. 確認メッセージで [OK] をクリックします。




=== vMedia 対応のサービスプロファイルテンプレートを作成します

vMedia を有効にしてサービスプロファイルテンプレートを作成するには、次の手順を実行します。

. UCS Manager に接続し、左側の [ サーバ ] をクリックします。
. サービスプロファイルテンプレート > ルート > サービステンプレート VM-Host-Infra-iSCSI-A を選択します
. [VM-Host-Infra-iSCSI-A] を右クリックし、 [ クローンの作成 ] を選択します。
. クローンに 'VM-Host-Infra-iSCSI-A-VM' という名前を付けます
. 新しく作成した VM-Host-Infra-iSCSI-A-VM を選択し、右側の [vMedia Policy] タブを選択します。
. Modify vMedia Policy をクリックします。
. ESXi-6 を選択します。7U1 - HTTP vMedia Policy （ HTTP vMedia ポリシー）を選択し、 OK をクリックします。
. [OK] をクリックして確定します。




=== サービスプロファイルを作成する

サービスプロファイルテンプレートからサービスプロファイルを作成するには、次の手順を実行します。

. Cisco UCS Manager に接続し、左側の [ サーバ ] をクリックします。
. ［ サーバー ］ > ［ サービスプロファイルテンプレート ］ > ［ ルート ］ > ［ サービステンプレート ］ を展開します。
. [ アクション ] で、 [ テンプレートからサービスプロファイルを作成 ] をクリックし、次の手順を実行します。
+
.. 命名プレフィックスとして「 `Site-01-Infra-0` 」を入力します。
.. 作成するインスタンスの数として「 2 」を入力します。
.. ルートを組織として選択します。
.. [OK] をクリックして、サービスプロファイルを作成します。
+
image:express-direct-attach-aff220-deploy_image52.png["エラー：グラフィックイメージがありません"]



. 確認メッセージで [OK] をクリックします。
. サービスプロファイル「 Site-01-Infra-01` 」および「 Site-01-Infra-02` 」が作成されていることを確認します。
+

NOTE: サービスプロファイルは、割り当てられたサーバプール内のサーバに自動的に関連付けられます。





== ストレージ構成パート 2 ：ブート LUN とイニシエータグループ



=== ONTAP ブートストレージのセットアップ



==== igroup を作成します

イニシエータグループ（ igroup ）を作成するには、次の手順を実行します。

. クラスタ管理ノードの SSH 接続から次のコマンドを実行します。
+
....
igroup create –vserver Infra-SVM –igroup VM-Host-Infra-01 –protocol iscsi –ostype vmware –initiator <vm-host-infra-01-iqn>
igroup create –vserver Infra-SVM –igroup VM-Host-Infra-02 –protocol iscsi –ostype vmware –initiator <vm-host-infra-02-iqn>
igroup create –vserver Infra-SVM –igroup MGMT-Hosts –protocol iscsi –ostype vmware –initiator <vm-host-infra-01-iqn>, <vm-host-infra-02-iqn>
....
+

NOTE: IQN 情報には、表 1 と表 2 の値を使用します。

. 作成した 3 つの igroup を表示するには、「 igroup show 」コマンドを実行します。




==== ブート LUN を igroup にマッピングします

ブート LUN を igroup にマッピングするには、次の手順を実行します。

. ストレージクラスタ管理 SSH 接続から、次のコマンドを実行します。 
+
....
lun map –vserver Infra-SVM –volume esxi_boot –lun VM-Host-Infra- A –igroup VM-Host-Infra-01 –lun-id 0lun map –vserver Infra-SVM –volume esxi_boot –lun VM-Host-Infra- B –igroup VM-Host-Infra-02 –lun-id 0
....




== VMware vSphere 6.7U1 導入手順

ここでは、 FlexPod Express 構成に VMware ESXi 6.7U1 をインストールする手順について説明します。手順が完了すると、ブートした 2 台の ESXi ホストがプロビジョニングされます。

VMware 環境に ESXi をインストールする方法はいくつかあります。これらの手順では、 Cisco UCS Manager に組み込まれている KVM コンソールと仮想メディア機能を使用して、リモートインストールメディアを個々のサーバにマッピングし、それらのブート LUN に接続する方法に焦点を当てています。



=== ESXi 6.7U1 用の Cisco カスタムイメージをダウンロードします

VMware ESXi カスタムイメージがダウンロードされていない場合は、次の手順を実行してダウンロードを完了します。

. 次のリンクをクリックします。 https://my.vmware.com/group/vmware/details?downloadGroup=OEM-ESXI67U1-CISCO&productId=742[VMware vSphere Hypervisor （ ESXi ） 6.7U1 。 ^ ]
. ユーザ ID とパスワードが必要です https://www.vmware.com/["VMware.com"^] このソフトウェアをダウンロードします。
. 「 .iso 」ファイルをダウンロードします。




==== Cisco UCS Manager の略

Cisco UCS IP KVM を使用すると、管理者はリモートメディアを介して OS のインストールを開始できます。IP KVM を実行するには、 Cisco UCS 環境にログインする必要があります。

Cisco UCS 環境にログインするには、次の手順を実行します。

. Web ブラウザを開き、 Cisco UCS クラスタアドレスの IP アドレスを入力します。このステップは、 Cisco UCS Manager アプリケーションを起動します。
. HTML の下の [UCS Manager の起動 ] リンクをクリックして、 HTML 5 UCS Manager GUI を起動します。
. セキュリティ証明書を承認するかどうかを尋ねられたら、必要に応じてを受け入れます。
. プロンプトが表示されたら、ユーザ名として「 admin 」と入力し、管理パスワードを入力します。
. Cisco UCS Manager にログインするには、 Login をクリックします。
. メインメニューの左側にある [ サーバー ] をクリックします。
. Servers > Service Profiles > root > 'VM-Host-Infra-01' を選択します
. [VM-Host-Infra-01] を右クリックし '[KVM Console] を選択します
. プロンプトに従って Java ベースの KVM コンソールを起動します。
. Servers > Service Profiles > root > 'VM-Host-Infra-02' を選択します
. [VM-Host-Infra-02] を右クリックします。KVM コンソールを選択します。
. プロンプトに従って Java ベースの KVM コンソールを起動します。




==== VMware ESXi のインストールをセットアップする

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

OS をインストールするサーバを準備するには、各 ESXi ホストで次の手順を実行します。

. KVM ウィンドウで、仮想メディアをクリックします。
. Activate Virtual Devices をクリックします。
. 暗号化されていない KVM セッションを許可するかどうかを尋ねられたら、必要に応じて受け入れます。
. [ 仮想メディア ] をクリックし、 [CD/DVD のマップ ] を選択します。
. ESXi インストーラの ISO イメージファイルを参照し、開くをクリックします。
. Map Device をクリックします。 
. KVM タブをクリックして ' サーバの起動を監視します


* ESXi のインストール *

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

VMware ESXi をホストの iSCSI ブート可能 LUN にインストールするには、各ホストで次の手順を実行します。

. [Boot Server] を選択し、 [OK] をクリックして、サーバを起動します。次に、もう一度 [OK] をクリックします。
. リブート時に、 ESXi インストールメディアがマシンで検出されます。表示されたブートメニューから ESXi インストーラを選択します。
. インストーラのロードが完了したら、 Enter キーを押してインストールを続行します。
. エンドユーザライセンス契約（ EULA ）を読んで同意します。F11 キーを押して確定し、続行します。
. ESXi のインストールディスクとして設定していた LUN を選択し、 Enter キーを押してインストールを続行します。
. 適切なキーボードレイアウトを選択し、 Enter キーを押します。
. ルートパスワードを入力して確定し、 Enter キーを押します。
. 選択したディスクが再パーティショニングされることを示す警告が表示されます。F11 キーを押してインストールを続行します。
. インストールが完了したら、 [Virtual Media] タブを選択し、 ESXi インストールメディアの横にある P マークをクリアします。はいをクリックします。
+

NOTE: ESXi のインストールイメージのマッピングを解除して、サーバがインストーラではなく ESXi でリブートされるようにする必要があります。

. インストールが完了したら、 Enter キーを押してサーバをリブートします。
. Cisco UCS Manager では、現在のサービスプロファイルを vMedia 以外のサービスプロファイルテンプレートにバインドして、 ESXi インストール ISO over HTTP をマウントできないようにします。




==== ESXi ホストの管理ネットワークをセットアップします

ホストの管理には、各 VMware ホストに管理ネットワークを追加する必要があります。VMware ホストの管理ネットワークを追加するには、各 ESXi ホストで次の手順を実行します。

ESXi ホスト VM-Host-Infra-01 と VM-Host-Infra-02

各 ESXi ホストから管理ネットワークにアクセスできるように設定するには、次の手順を実行します。

. サーバーの再起動が完了したら、 F2 キーを押してシステムをカスタマイズします。
. root としてログインし ' 対応するパスワードを入力し 'Enter キーを押してログインします
. [ トラブルシューティングオプション ] を選択し、 Enter キーを押します。
. [Enable ESXi Shell] を選択し、 Enter キーを押します。
. SSH を有効にするを選択し、 Enter キーを押します。
. Esc キーを押して、トラブルシューティングオプションメニューを終了します。
. Configure Management Network （管理ネットワークの設定）オプションを選択し、 Enter キーを押します。
. [ ネットワークアダプタ ] を選択し、 Enter キーを押します。
. [ ハードウェアラベル ] フィールドの番号が [ デバイス名 ] フィールドの番号と一致していることを確認します。
. Enter キーを押します。
+
image:express-direct-attach-aff220-deploy_image53.png["エラー：グラフィックイメージがありません"]

. VLAN （オプション）オプションを選択し、 Enter キーを押します。
. 「 <ib-mgmt-vlan-id> 」を入力し、 Enter キーを押します。
. IPv4 Configuration （ IPv4 設定）を選択し、 Enter を押します。
. スペースバーを使用して、静的 IPv4 アドレスとネットワーク設定を設定オプションを選択します。
. 最初の ESXi ホストを管理するための IP アドレスを入力します。
. 最初の ESXi ホストのサブネットマスクを入力します。
. 最初の ESXi ホストのデフォルトゲートウェイを入力します。
. Enter キーを押して、 IP 設定の変更を確定します。
. DNS Configuration オプションを選択し、 Enter キーを押します。
+

NOTE: IP アドレスは手動で割り当てられるため、 DNS 情報も手動で入力する必要があります。

. プライマリ DNS サーバの IP アドレスを入力します。
. オプション：セカンダリ DNS サーバの IP アドレスを入力します。
. 最初の ESXi ホストの FQDN を入力します。
. Enter キーを押して、 DNS 設定の変更を確定します。
. Esc キーを押して、 Configure Management Network （管理ネットワークの設定）メニューを終了します。
. 管理ネットワークのテストを選択して管理ネットワークが正しく設定されていることを確認し、 Enter キーを押します。
. Enter キーを押してテストを実行し、テストが完了したら Enter キーを再度押し、失敗した場合は環境を確認します。
. Configure Management Network （管理ネットワークの設定）をもう一度選択し、 Enter キーを押します。
. IPv6 設定オプションを選択し、 Enter キーを押します。
. スペースバーを使用して、 [Disable IPv6 (restart required)] を選択し、 Enter キーを押します。
. Esc キーを押して、 Configure Management Network サブメニューを終了します。
. Y キーを押して変更を確認し、 ESXi ホストをリブートします。




==== VMware ESXi ホストの VMkernel ポート vmk0 MAC アドレスのリセット（オプション）

ESXi ホスト VM-Host-Infra-01 と VM-Host-Infra-02

デフォルトでは、管理 VMkernel ポート vmk0 の MAC アドレスは、配置されているイーサネットポートの MAC アドレスと同じです。ESXi ホストのブート LUN が異なる MAC アドレスを持つ別のサーバに再マッピングされた場合、 vmk0 では ESXi システム設定がリセットされないかぎり、割り当てられた MAC アドレスが保持されるため、 MAC アドレスの競合が発生します。vmk0 の MAC アドレスを、 VMware が割り当てたランダムな MAC アドレスにリセットするには、次の手順を実行します。

. ESXi コンソールメニューのメイン画面で、 Ctrl+Alt+F1 キーを押して VMware コンソールのコマンドラインインターフェイスにアクセスします。UCSM KVM では、静的マクロのリストに Ctrl-Alt-F1 が表示されます。
. root としてログインします。
. 「 esxcfg-vmknic – l 」と入力して、インタフェース vmk0 の詳細な一覧を表示します。vmk0 は、管理ネットワークのポートグループの一部にする必要があります。vmk0 の IP アドレスおよびネットマスクに注意してください。
. vmk0 を削除するには、次のコマンドを入力します。
+
....
esxcfg-vmknic –d “Management Network”
....
. ランダム MAC アドレスを使用して vmk0 を再び追加するには、次のコマンドを入力します。
+
....
esxcfg-vmknic –a –i <vmk0-ip> -n <vmk0-netmask> “Management Network””.
....
. vmk0 がランダム MAC アドレスで再び追加されていることを確認します
+
....
esxcfg-vmknic –l
....
. コマンド・ライン・インターフェイスからログアウトするには、「 exit 」と入力します。
. ESXi コンソールメニューインターフェイスに戻るには、 Ctrl+Alt+F2 を押します。




==== VMware ホストクライアントを使用して VMware ESXi ホストにログインします

ESXi ホスト VM-Host-Infra-01

VMware Host Client を使用して VM-Host-Infra-01 ESXi ホストにログインするには、次の手順を実行します。

. 管理ワークステーションで Web ブラウザを開き 'VM-Host-Infra-01' 管理 IP アドレスに移動します
. ［ VMware ホストクライアントを開く ］ をクリックします。
. ユーザ名に「 root 」と入力します。
. root パスワードを入力します。
. ログインをクリックして接続します。
. この手順を繰り返して 'VM-Host-Infra-02' に別のブラウザタブまたはウィンドウでログインします




==== Cisco Virtual Interface Card （ VIC; 仮想インターフェイスカード）用の VMware ドライバのインストール

次の VMware VIC ドライバのオフラインバンドルをダウンロードして、管理ワークステーションに展開します。

* nenic ドライババージョン 1.0.25.0




==== ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

ESXi ホスト VM-Host-Infra-01 および VM-Host-Infra-02 に VMware VIC ドライバをインストールするには、次の手順を実行します。

. 各ホストクライアントで、 Storage （ストレージ）を選択します。
. datastore1 を右クリックし、 Browse を選択します。
. データストアブラウザで、 [ アップロード ] をクリックします。
. ダウンロードした VIC ドライバの保存先に移動し、 VMW-ESX-6.7.0-nenic-1.0.25.0 -offline_bundle-11271332.zip を選択します。
. データストアブラウザで、 [ アップロード ] をクリックします。
. [ 開く ] をクリックして、このファイルを datastore1 にアップロードします。
. 両方の ESXi ホストにファイルがアップロードされていることを確認してください。
. 各ホストがメンテナンスモードになっていない場合は、メンテナンスモードにします。
. 各 ESXi ホストへは、シェル接続または putty 端末から ssh を使用して接続します。
. root パスワードを使用して root としてログインします。
. 各ホストで次のコマンドを実行します。
+
....
esxcli software vib update -d /vmfs/volumes/datastore1/VMW-ESX-6.7.0-nenic-1.0.25.0-offline_bundle-11271332.zip
reboot
....
. 再起動が完了したら各ホストでホストクライアントにログインし、メンテナンスモードを終了します。




==== VMkernel ポートおよび仮想スイッチを設定します

ESXi ホスト VM-Host-Infra-01 と VM-Host-Infra-02

ESXi ホスト上の VMkernel ポートおよび仮想スイッチを設定するには、次の手順を実行します。

. ホストクライアントで、左側の [ ネットワーク ] を選択します。
. 中央のペインで、 [Virtual switches] タブを選択します。
. vSwitch0 を選択します。
. [ 設定の編集 ] を選択します
. MTU を 9000 に変更します。
. NIC チーミングを展開します。
. フェイルオーバー順序（ Failover order ）セクションで、 vmnic1 を選択し、アクティブとしてマーク（ Mark active ）をクリックします。
. vmnic1 のステータスがアクティブになっていることを確認します。
. [ 保存 ] をクリックします .
. 左側の [ ネットワーク ] を選択します。
. 中央のペインで、 [Virtual switches] タブを選択します。
. iScsiBootvSwitch を選択します。
. [ 設定の編集 ] を選択します
. MTU を 9000 に変更します
. [ 保存 ] をクリックします .
. [VMkernel NICs] タブを選択します。
. 「 vmk1 iScsiBootPG 」を選択します。
. [ 設定の編集 ] を選択します
. MTU を 9000 に変更します。
. IPv4 設定を展開し、 IP アドレスを UCS iSCSI-IP-Pool-A の外部のアドレスに変更します
+

NOTE: Cisco UCS iSCSI IP プールアドレスを再割り当てする必要がある場合に IP アドレスの競合を回避するには、 iSCSI VMkernel ポートに対して同じサブネット内の異なる IP アドレスを使用することを推奨します。

. [ 保存 ] をクリックします .
. [Virtual switches] タブを選択します。
. Add standard virtual switch を選択します。
. vSwitch 名には「 iScsciBootvSwitch -B 」という名前を付けます。
. MTU を 9000 に設定します。
. [Uplink 1] ドロップダウンメニューから [vmnic3] を選択します。
. 追加をクリックします。
. 中央のペインで、 [VMkernel NICs] タブを選択します。
. Add VMkernel NIC を選択します
. 新しいポートグループ名として、 iScsiBootPG-B を指定します
. 仮想スイッチに [iSciBootvSwitch -B ] を選択します。
. MTU を 9000 に設定します。VLAN ID は入力しないでください。
. IPv4 設定では Static を選択し、 Configuration 内で Address と Subnet Mask を指定するオプションを展開します。
+

NOTE: IP アドレスの競合を避けるため、 Cisco UCS iSCSI IP プールアドレスを再割り当てする必要がある場合は、 iSCSI VMkernel ポートに対して同じサブネット内の異なる IP アドレスを使用することを推奨します。

. Create をクリックします。 .
. 左側で、 [ ネットワーク ] を選択し、 [ ポートグループ ] タブを選択します。
. 中央のペインで、 [VM Network] を右クリックし、 [ 削除 ] を選択します。
. Remove をクリックして、ポートグループの削除を完了します。
. 中央のペインで、 Add port group （ポートグループの追加）を選択します。
. ポートグループに「 Management Network 」という名前を付け、 VLAN ID フィールドに「 <ib-mgmt-vlan-id> 」と入力して、仮想スイッチ vSwitch0 が選択されていることを確認します。
. [Add] をクリックして、 IB-MGMT ネットワークの編集を終了します。
. 上部で、 [VMkernel NICs] タブを選択します。
. Add VMkernel NIC をクリックします。
. 新規ポートグループの場合は、 VMotion と入力します。
. 仮想スイッチの場合は、 vSwitch0 を選択します。
. VLAN ID に「 <VMotion-vlan-id> 」と入力します。
. MTU を 9000 に変更します。
. 静的 IPv4 設定を選択し、 IPv4 設定を展開します。
. ESXi ホストの vMotion IP アドレスとネットマスクを入力します。
. vMotion スタック TCP/IP スタックを選択します。
. Services （サービス）で vMotion （ vMotion ）を選択
. Create をクリックします。 .
. Add VMkernel NIC をクリックします。
. 新しいポートグループの場合は、 nfs_Share と入力します。
. 仮想スイッチの場合は、 vSwitch0 を選択します。
. VLAN ID に「 <infra-nfs-vlan-id> 」と入力します
. MTU を 9000 に変更します。
. 静的 IPv4 設定を選択し、 IPv4 設定を展開します。
. ESXi ホストインフラの NFS IP アドレスとネットマスクを入力します。
. サービスは選択しないでください。
. Create をクリックします。 .
. 仮想スイッチタブを選択して、 vSwitch0 を選択します。vSwitch0 VMkernel NIC のプロパティは、次の例のように設定します。
+
image:express-direct-attach-aff220-deploy_image54.png["エラー：グラフィックイメージがありません"]

. [VMkernel NICs] タブを選択して、設定済みの仮想アダプタを確認します。次の例のようなアダプタが表示されます。
+
image:express-direct-attach-aff220-deploy_image55.png["エラー：グラフィックイメージがありません"]





==== iSCSI マルチパスをセットアップします

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

ESXi ホスト VM-Host-Infra-01 および VM-Host-Infra-02 で iSCSI マルチパスを設定するには、次の手順を実行します。

. 各ホストクライアントで、左側の [ ストレージ ] を選択します。
. 中央のペインで、 [ アダプタ ] をクリックします。
. iSCSI ソフトウェアアダプタを選択し、 Configure iSCSI （ iSCSI の設定）をクリックします。
+
image:express-direct-attach-aff220-deploy_image56.png["エラー：グラフィックイメージがありません"]

. [ 動的ターゲット ] で、 [ 動的ターゲットの追加 ] をクリックします。
. IP アドレスに「 iscsi_dlif01a 」と入力します。
. これらの IP アドレスの入力を繰り返します： 'iSCSI_lif01b'iSCSI_lif02a'iSCSI_lif02b'
. [Save Configuration] をクリックします。
+
image:express-direct-attach-aff220-deploy_image57.png["エラー：グラフィックイメージがありません"]

+
「 iscsi_lif 」の IP アドレスをすべて取得するには、 NetApp ストレージ・クラスタ管理インターフェイスにログインし、「 network interface show 」コマンドを実行します。

+

NOTE: ホストが自動的にストレージアダプタとターゲットを再スキャンし、静的ターゲットに追加します。





==== 必要なデータストアをマウント

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

必要なデータストアをマウントするには、各 ESXi ホストで次の手順を実行します。

. ホスト・クライアントで ' 左側の Storage を選択します
. 中央のペインで、 [Datastores] を選択します。
. 中央のペインで、 New Datastore （新規データストア）を選択して新しいデータストアを追加します。
. [ 新規データストア ] ダイアログボックスで、 [ NFS データストアのマウント ] を選択し、 [ 次へ ] をクリックします。
+
image:express-direct-attach-aff220-deploy_image58.png["エラー：グラフィックイメージがありません"]

. [Provide NFS Mount Details] ページで、次の手順を実行します。
+
.. データストア名として「 infra_datastore_1 」と入力します。
.. NFS サーバの「 NFS_lif01_a 」 LIF の IP アドレスを入力します。
.. NFS 共有の場合は '/infra_datastore_1' と入力します
.. NFS のバージョンは NFS 3 のままにします。
.. 次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image59.png["エラー：グラフィックイメージがありません"]



. 完了をクリックします。これで、データストアがデータストアのリストに表示されます。
. 中央のペインで、 New Datastore （新規データストア）を選択して新しいデータストアを追加します。
. New Datastore （新規データストア）ダイアログボックスで、 Mount NFS Datastore （ NFS データストアのマウント）を選択し、 Next （次へ）をクリック
. [Provide NFS Mount Details] ページで、次の手順を実行します。
+
.. データストア名として「 infra_datastore_2 」と入力します。
.. NFS サーバの「 nfs_lif02_a 」 LIF の IP アドレスを入力します。
.. NFS 共有の場合は '/infra_datastore_2' と入力します
.. NFS のバージョンは NFS 3 のままにします。
.. 次へをクリックします。


. 完了をクリックします。これで、データストアがデータストアのリストに表示されます。
+
image:express-direct-attach-aff220-deploy_image60.jpeg["エラー：グラフィックイメージがありません"]

. 両方の ESXi ホストに両方のデータストアをマウントします。




==== ESXi ホストで NTP を設定

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

ESXi ホストで NTP を設定するには、各ホストで次の手順を実行します。

. ホストクライアントから、左側の [ 管理 ] を選択します。
. 中央のウィンドウ枠で、 [ 時刻と日付 ] タブを選択します。
. 設定の編集をクリックします。
. [ ネットワークタイムプロトコルを使用する (NTP クライアントを有効にする )] が選択されていることを確認します。
. ドロップダウンメニューを使用して、 Start （開始）および Stop with Host （ホストで停止）を選択します。
. 2 つの Nexus スイッチの NTP アドレスを、カンマで区切って NTP サーバボックスに入力します。
+
image:express-direct-attach-aff220-deploy_image61.png["エラー：グラフィックイメージがありません"]

. Save をクリックして、設定の変更を保存します。
. Actions > NTP service > Start の順に選択します。
. NTP サービスが実行中で、クロックが正しい時刻に設定されたことを確認します
+

NOTE: NTP サーバの時間はホストの時間とは多少異なる場合があります。





==== ESXi ホストのスワップを設定

ESXi は VM-Host-Infra-01 と VM-Host-Infra-02 をホストします

ESXi ホストでホストのスワップを設定するには、各ホストで次の手順を実行します。

. 左側のナビゲーションペインで、 [ 管理 ] をクリックします。右側のペインで System （システム）を選択し、 Swap （交換）をクリックします。
+
image:express-direct-attach-aff220-deploy_image62.png["エラー：グラフィックイメージがありません"]

. 設定の編集をクリックします。データストアのオプションから 'infra_swap' を選択します
+
image:express-direct-attach-aff220-deploy_image63.png["エラー：グラフィックイメージがありません"]

. [ 保存 ] をクリックします .




==== NetApp NFS Plug-in 1.1.2 for VMware VAAI をインストールします

NetApp NFS Plug-in 1 をインストールします。1.2 VMware VAAI の場合は、次の手順を実行します。

. NetApp NFS Plug-in for VMware VAAI をダウンロードします。
+
.. にアクセスします https://mysupport.netapp.com/NOW/download/software/nfs_plugin_vaai_esxi6/1.1.2/["ネットアップのソフトウェアダウンロードページ"^]。
.. 下にスクロールして、 NetApp NFS Plug-in for VMware VAAI をクリックします。
.. ESXi プラットフォームを選択します。
.. 最新のプラグインのオフラインバンドル（ .zip ）またはオンラインバンドル（ .vib ）をダウンロードします。


. NetApp NFS Plug-in for VMware VAAI ONTAP は IMT 9.5 への対応が保留中であり、相互運用性の詳細は NetApp IMT に近日中に公開されます。
. ESX CLI を使用して、 ESXi ホストにプラグインをインストールします。
. ESXi ホストをリブートします。




== VMware vCenter Server 6.7 をインストールする

このセクションでは、 FlexPod 構成に VMware vCenter Server 6.7 をインストールする詳細な手順について説明します。


NOTE: FlexPod Express では、 VMware vCenter Server Appliance （ VCSA ）を使用します。



=== VMware vCenter Server Appliance をインストールする

vCSA をインストールするには、次の手順を実行します。

. vCSA をダウンロードします。ESXi ホストの管理時に Get vCenter Server アイコンをクリックして、ダウンロードリンクにアクセスします。
+
image:express-direct-attach-aff220-deploy_image64.png["エラー：グラフィックイメージがありません"]

. vCSA を VMware サイトからダウンロードします。
+

NOTE: インストール可能な Microsoft Windows vCenter Server がサポートされますが、 VMware では新しい導入に vCSA を推奨します。

. ISO イメージをマウントします。
. 「 VCSA -ui-sinstaller 」 > 「 win32 」ディレクトリに移動します。「 installer.exe 」をダブルクリックします。
. [ インストール ] をクリックします
. [ はじめに ] ページで [ 次へ ] をクリックします。
. EULA に同意します。
. 展開タイプとして、 Embedded Platform Services Controller を選択します。
+
image:express-direct-attach-aff220-deploy_image65.png["エラー：グラフィックイメージがありません"]

+
必要に応じて、 FlexPod Express 解決策の一部として、外部プラットフォームサービスコントローラの導入もサポートされます。

. アプライアンス導入ターゲットページで、導入した ESXi ホストの IP アドレス、ルートユーザ名、および root パスワードを入力します。次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image66.png["エラー：グラフィックイメージがありません"]

. vCSA に VM 名および vCSA に使用するルートパスワードとして VCSA を入力して、アプライアンス VM を設定します。次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image67.png["エラー：グラフィックイメージがありません"]

. 環境に最も適した導入サイズを選択してください。次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image68.png["エラー：グラフィックイメージがありません"]

. 「 infra_datastore_1' 」データストアを選択します。次へをクリックします。
+
image:express-direct-attach-aff220-deploy_image69.png["エラー：グラフィックイメージがありません"]

. [Configure Network Settings] ページで次の情報を入力し、 [Next] をクリックします。
+
.. ネットワークとして MGMT-Network を選択します。
.. vCSA に使用する FQDN または IP を入力します。
.. 使用する IP アドレスを入力します。
.. 使用するサブネットマスクを入力します。
.. デフォルトゲートウェイを入力します。
.. DNS サーバを入力します。
+
image:express-direct-attach-aff220-deploy_image70.png["エラー：グラフィックイメージがありません"]



. 「ステージ 1 を完了する準備ができました」ページで、入力した設定が正しいことを確認します。完了をクリックします。
+
vCSA がインストールされます。このプロセスには数分かかります。

. ステージ 1 が完了すると、完了したことを示すメッセージが表示されます。「続行」をクリックしてステージ 2 の設定を開始します。
+
image:express-direct-attach-aff220-deploy_image71.png["エラー：グラフィックイメージがありません"]

. 「ステージ 2 の紹介」ページで、「次へ」をクリックします。
. NTP サーバのアドレスとして「 \\<<var_ntp_id>> 」と入力します。複数の NTP IP アドレスを入力できます。
+
vCenter Server の高可用性機能を使用する場合は、 SSH アクセスが有効になっていることを確認してください。

. SSO ドメイン名、パスワード、およびサイト名を設定します。次へをクリックします。
+
特に 'vSpher.local' ドメイン名から外れる場合は ' これらの値を参考にしてください

. 必要に応じて、 VMware カスタマーエクスペリエンスプログラムに参加します。次へをクリックします。
. 設定の概要を確認します。[ 完了 ] をクリックするか、 [ 戻る ] ボタンを使用して設定を編集します。
. インストールの開始後に、インストールを一時停止または終了できないことを示すメッセージが表示されます。[OK] をクリックして続行します。
+
アプライアンスの設定が続行されます。これには数分かかります。

+
セットアップが正常に完了したことを示すメッセージが表示されます。

+

NOTE: インストーラが vCenter Server にアクセスするために提供するリンクはクリック可能です。





==== VMware vCenter Server 6.7 および vSphere クラスタリングを設定する

VMware vCenter Server 6.7 および vSphere クラスタリングを設定するには、次の手順を実行します。

. https://\<<FQDN または IP of vCenter >> /vsphere-client/ に移動します。
. vSphere Client の起動をクリックします。
. vCSA のセットアッププロセスで入力したユーザ名 administrator@vsphere.loca と SSO パスワードを使用してログインします。
. vCenter 名を右クリックし、 New Datacenter を選択します。
. データセンターの名前を入力し、 [OK] をクリックします。


* vSphere クラスタを作成 *

vSphere クラスタを作成するには、次の手順を実行します。

. 新しく作成したデータセンターを右クリックし、 [New Cluster] を選択します。
. クラスタの名前を入力します。
. DRS と vSphere HA のオプションを選択して有効にします。
. [OK] をクリックします。
+
image:express-direct-attach-aff220-deploy_image72.png["エラー：グラフィックイメージがありません"]



* ESXi ホストをクラスタに追加 *

ESXi ホストをクラスタに追加するには、次の手順を実行します。

. クラスタの Actions （アクション）メニューで Add Host （ホストの追加）を選択します。
+
image:express-direct-attach-aff220-deploy_image73.png["エラー：グラフィックイメージがありません"]

. ESXi ホストをクラスタに追加するには、次の手順を実行します。
+
.. ホストの IP または FQDN を入力します。次へをクリックします。
.. root ユーザ名とパスワードを入力します。次へをクリックします。
.. Yes をクリックして、ホストの証明書を VMware 証明書サーバによって署名された証明書に置き換えます。
.. [Host Summary] ページで [Next] をクリックします。
.. 緑の + アイコンをクリックして、 vSphere ホストにライセンスを追加します。
+

NOTE: この手順は、必要に応じてあとで実行できます。

.. [ 次へ ] をクリックして、ロックダウンモードを無効のままに
.. [VM の場所 ] ページで [ 次へ ] をクリックします。
.. [Ready to Complete] ページを確認します。[ 戻る ] ボタンを使用して変更を行うか、 [ 完了 ] を選択します。


. Cisco UCS ホスト B に対して手順 1 と 2 を繰り返します
+
FlexPod 構成にホストを追加する場合は、この手順を実行する必要があります。





==== ESXi ホストにコアダンプを設定します

iSCSI ブートホスト用の ESXi ダンプコレクタのセットアップ

VMware iSCSI ソフトウェアイニシエータを使用して iSCSI でブートされた ESXi ホストは、 vCenter の一部である ESXi ダンプコレクタにコアダンプを実行するように設定する必要があります。ダンプコレクタは、 vCenter Appliance ではデフォルトで有効になっていません。この手順は、 vCenter の導入セクションの最後で実行する必要があります。ESXi Dump Collector をセットアップするには、次の手順を実行します。

. vSphere Web Client に mailto ： administrator@vsphere.loca l [administrator@vsphere.loca l^] としてログインし、 [ ホーム ] を選択します。
. 中央のペインで、システム構成をクリックします。
. 左側のペインで、 [ サービス ] を選択します。
. [Services] で、 [VMware vSphere ESXi Dump Collector] をクリックします。
. 中央のペインで、緑の開始アイコンをクリックしてサービスを開始します。
. [ アクション ] メニューの [ スタートアップの種類の編集 ] をクリックします。
. 自動を選択します。
. [OK] をクリックします。
. SSH を使用して、各 ESXi ホストに root として接続します。
. 次のコマンドを実行します。
+
....
esxcli system coredump network set –v vmk0 –j <vcenter-ip>
esxcli system coredump network set –e true
esxcli system coredump network check
....
+
最後のコマンドを実行すると ' 構成された netdump サーバが動作していることを確認しましたというメッセージが表示されます

+

NOTE: FlexPod Express にホストを追加する場合は、このプロセスを完了する必要があります。


