---
sidebar: sidebar 
permalink: healthcare/ehr-epic-performance_test_methodology.html 
keywords: test, methodology, generationio, tool, genio, storage, harware, software, vms 
summary: Epic で GenerationIO ツール（ Genio ）を使用して、ストレージが本番環境に対応しているかどうかを検証しました。このテストでは、ストレージを制限までプッシュし、要件が失敗するまで徐々に稼働させてストレージコントローラのヘッドルームを決定することで、パフォーマンスに焦点を当てます。 
---
= テスト方法
:hardbreaks:
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== テスト計画

Epic で GenerationIO ツール（ Genio ）を使用して、ストレージが本番環境に対応しているかどうかを検証しました。このテストでは、ストレージを制限までプッシュし、要件が失敗するまで徐々に稼働させてストレージコントローラのヘッドルームを決定することで、パフォーマンスに焦点を当てます。

ここで実施するテストでは、ヘッドルームの特定と、アダプティブ Quality of Service （ AQOS ）を使用した重要な Epic ワークロードの保護に焦点を当てています。AFF A300 のテストでは、 2 台のサーバを両方にロードした Genio で使用して、ストレージコントローラ上の I/O を処理します。3 台のサーバをすべての 3 台にロードされた Genio とともに使用して、 AFF A700 ストレージコントローラ上の I/O をドライブします。サーバのパフォーマンス制限のために 3 台のサーバが使用され、 AFF A700 には 3 台のサーバが必要です。



== テスト環境



=== ハードウェアとソフトウェア：

今回の調査では、 Cisco UCS B200-M5s 上で実行される VMware ESXi 6.5 で 3 台の Red Hat Linux 仮想マシン（ VM ）を構成しました。サーバ側で 16Gb FC を使用し、ストレージ側で 16GB FC を使用して、 ESXi ホストを AFF ストレージコントローラノードに接続しました。AFF A700 ノードは、ネットアップのケーブル配線のベストプラクティスに従って、 3.8TB SSD の DS2446 ディスクシェルフの 1 台に接続しました。

以下の 3 つの表に、 Epic パフォーマンステスト構成で使用したハードウェアとソフトウェアのコンポーネントを示します。

次の表に、 Epic Test のハードウェアおよびソフトウェアコンポーネントを示します。

|===
| ハードウェアおよびソフトウェアコンポーネント | 詳細 


| VM 用のオペレーティングシステム | RHEL 7.4 VM 


| サーバブレード上のオペレーティングシステム | VMware ESXi 6.5 の場合 


| 物理サーバ | Cisco UCS B200 M5 × 3 


| サーバあたりのプロセッサ数 | 20 コア Intel Xeon Gold 6148 2.4GHz x 2 


| サーバあたりの物理メモリ | 768GB 


| FC ネットワーク | 16Gb FC 、マルチパス対応 


| FC HBA | Cisco UCS VIC 1340 上の FC vHBA 


| クラスタ管理用の専用パブリック 1GbE ポート | Intel 1350GbE ポート × 2 


| 16Gb FC スイッチ | Cisco MDS 9148s 


| 40GbE スイッチ | Cisco Nexus 9332 スイッチ 
|===
次の表に、 NetApp AFF A700 および AFF A300 ストレージシステムのハードウェアとソフトウェアを示します。

|===
| ハードウェアおよびソフトウェアコンポーネント | AFF A700 の詳細 | AFF A300 の詳細 


| ストレージシステム | ハイアベイラビリティ（ HA ）アクティブ / アクティブペアとして構成された AFF A700 コントローラ | AFF A300 コントローラがハイアベイラビリティ（ HA ）アクティブ / アクティブペアとして構成されている場合 


| ONTAP バージョン | 9.4 | 9.5 


| ドライブの総数 | 36 | 24 


| ドライブサイズ | 3.8TB | 3.8TB 


| ドライブタイプ | SSD の場合 | SSD の場合 


| FC ターゲットポート | 16Gb ポート × 8 （ノードあたり 4 つ） | 16Gb ポート × 8 （ノードあたり 4 つ） 


| イーサネットポート | 10Gb ポート × 4 （ノードあたり 2 つ） | 10Gb ポート × 4 （ノードあたり 2 つ） 


| Storage Virtual Machine （ SVM ） | 両方のノードのアグリゲートに 1 つの SVM | 両方のノードのアグリゲートに 1 つの SVM 


| イーサネット論理インターフェイス（ LIF ） | 1Gb 管理 LIF × 4 （ノードあたり 2 つ。別々のプライベート VLAN に接続） | 1Gb 管理 LIF × 4 （ノードあたり 2 つ。別々のプライベート VLAN に接続） 


| FC LIF | 16Gb データ LIF × 4 | 16Gb データ LIF × 4 
|===
次の表に、 NetApp AFF A700 と AFF A300 ストレージシステムのレイアウトを示します。

|===
| ストレージレイアウト | AFF A700 の詳細 | AFF A300 の詳細 


| SVM | Epic アプリケーションデータベース対応の SVM が 1 つ | Epic アプリケーションデータベース対応の SVM が 1 つ 


| アグリゲート | それぞれ 2 台の 20TB | それぞれ 2 つの 30TB です 


| 本番用のボリューム | RHEL VM あたり 342GB のボリューム × 16 | RHEL VM あたり 16 個の 512GB ボリューム 


| 本番用の LUN | 307GB LUN × 16 、ボリュームあたり 1 個 | 460GB の LUN を 16 個、ボリュームごとに 1 個 


| ジャーナル用のボリューム | RHEL VM ごとに 2 つの 95Gb ボリューム | RHEL VM ごとに 240GB のボリュームを 2 個 


| ジャーナル用の LUN | 75GB の LUN が 2 つ、ボリュームごとに 1 つ | 190Gb LUN 2 個（ボリュームごとに 1 個 
|===